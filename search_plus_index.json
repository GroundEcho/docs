{"./":{"url":"./","title":"Introduction","keywords":"","body":"TKEStack - Tencent Kubernetes Engine Stack 在线文档地址：https://tkestack.github.io/docs/ TKEStack 是一个开源项目，为在生产环境中部署容器的组织提供一个统一的容器管理平台。 TKEStack 可以简化部署和使用 Kubernetes，满足 IT 要求，并增强 DevOps 团队的能力。 特点 统一集群管理 提供 Web 控制台和命令行客户端，用于集中管理多个 Kubernetes 集群 可与现有的身份验证机制集成，包括 LDAP，Active Directory，front proxy 和 public OAuth providers（例如GitHub） 统一授权管理，不仅在集群管理级别，甚至在Kubernetes资源级别 多租户支持，包括团队和用户对容器、构建和网络通信的隔离 应用程序工作负载管理 提供直观的UI界面，以支持可视化、YAML导入、其他资源创建和编辑方法，使用户无需预先学习所有Kubernetes概念即可运行容器 抽象的项目级资源容器，以支持跨多个集群的多个名称空间管理和部署应用程序 运维管理 集成的系统监控和应用程序监控 支持对接外部存储，以实现持久化Kubernetes事件和审计日志 限制，跟踪和管理平台上的开发人员和团队 插件支持和管理 Authentication identity provider 插件 Authorization provider 插件 事件持久化存储插件 系统和应用程序日志持久化存储插件 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-26 16:00:33 "},"zh/1-产品部署指南/":{"url":"zh/1-产品部署指南/","title":"产品部署指南","keywords":"","body":"产品部署指南 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:02:59 "},"zh/1-产品部署指南/1-1-产品部署架构.html":{"url":"zh/1-产品部署指南/1-1-产品部署架构.html","title":"产品部署架构","keywords":"","body":"产品部署架构 总体架构 TKEStack 产品架构如下图所示： 架构说明 TKEStack 采用了 Kubernetes on Kubernetes 的设计理念。即节点仅运行 Kubelet 进程，其他组件均采用容器化部署，由 Kubernetes 进行管理。 架构上分为 Global 集群和业务集群。Global 集群运行整个容器服务开源版平台自身所需要的组件，业务集群运行用户业务。在实际的部署过程中，可根据实际情况进行调整。 模块说明 Installer: 运行 tke-installer 安装器的节点，用于提供 Web UI 指导用户在 Global 集群部署TKEStacl控制台； Global Cluster: 运行的 TKEStack 控制台的 Kubernetes 集群； Cluster: 运行业务的 Kubernetes 集群，可以通过 TKEStack 控制台创建或导入； Auth: 权限认证组件，提供用户鉴权、权限对接相关功能； Gateway: 网关组件，实现集群后台统一入口、统一鉴权相关的功能，并运行控制台的 Web 界面服务； Platform: 集群管理组件，提供 Global 集群管理多个业务集群相关功能； Business: 业务管理组件，提供平台业务管理相关功能的后台服务； Network Controller：网络服务组件，支撑 Galaxy 网络功能； Monitor: 监控服务组件，提供监控采集、上报、告警相关服务； Notify: 通知功能组件，提供消息通知相关的功能； Registry: 镜像服务组件，提供平台镜像仓库服务； TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:43:41 "},"zh/1-产品部署指南/1-2-部署环境要求.html":{"url":"zh/1-产品部署指南/1-2-部署环境要求.html","title":"部署环境要求","keywords":"","body":"部署环境要求 硬件要求 特别注意： 安装的时候，至少需要一个Installer节点和一个作为Global集群的master节点。业务集群的节点是在部署完Global集群的master节点之后再添加的。 Installer节点：是单独的用作安装的节点，不能作为Global集群的节点使用。因为在安装Global集群时，需要多次重启docker，此时如果Global集群里面有Installer节点，重启docker会中断Global集群的安装。该节点需要一台系统盘100G的机器，系统盘要保证剩余50GB可用的空间，all-in-one模式还需要更多的空间。 Global集群：至少需要一台8核16G内存，100G系统盘的机器。 最小化部署配置： 安装/业务集群 节点/集群 CPU核数 内存 系统盘 数量 安装 Installer节点 1 2G 100G 1 Global集群 8 16G 100G 1 业务集群 Master & etcd 4 8G 100G 1 Node 8 16G 100G 3 推荐配置： 安装/业务集群 节点/集群 CPU核数 内存 系统盘 数量 安装 Installer节点 1 2G 100G 1 Global集群 8 16G 100G SSD 3 业务集群 Master & etcd 16 32G 300G SSD 3 Node 16 32G 系统盘：100G数据盘：300G （/var/lib/docker） >3 软件要求 注意，以下要求针对所有节点。 需求项 具体要求 参考（以CentOS7.6为例） 操作系统 Ubuntu 16.04/18.04 LTS (64-bit) CentOS Linux 7.6 (64-bit)Tencent Linux 2.2 cat /etc/redhat-release kernel 版本 >= Kernel 3.10.0-957.10.1.el7.x86_64 uname -sr ssh 确保 Installer 节点及其容器Global 集群节点及其容器业务集群节点及其容器之间能够 ssh 互联 确保在添加所有节点时，IP和密码输入正确 swap 关闭。如果不满足，系统会有一定几率出现 io 飙升，造成 docker 卡死。kubelet 会启动失败(可以设置 kubelet 启动参数 --fail-swap-on 为 false 关闭 swap 检查) swapoff -ased -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab 防火墙 关闭。Kubernetes 官方要求 systemctl stop firewalld && systemctl disable firewalld 端口 所有节点防火墙必须放通放通 SSH（默认22）、80、8080、443、6443 端口 firewall-cmd --zone=public --add-port=80/tcp --permanent SELinux 关闭。Kubernetes 官方要求，否则 kubelet 挂载目录时可能报错 Permission denied setenforce 0 sed -i \"s/SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config 时区 所有服务器时区必须统一，建议设置为 Asia/Shanghai timedatectl set-timezone Asia/Shanghai 时间同步 etcd 集群各机器需要时间同步，可以利用chrony 用于系统时间同步；所有服务器要求时间必须同步，误差不得超过 2 秒 yum install -y chronyd systemctl enable chronyd && systemctl start chronyd TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:44:12 "},"zh/1-产品部署指南/1-3-安装步骤.html":{"url":"zh/1-产品部署指南/1-3-安装步骤.html","title":"安装步骤","keywords":"","body":"安装步骤 1. 需求检查 仔细检查每个节点的硬件和软件需求：installation requirements 2. Installer安装 为了简化平台安装过程，容器服务开源版基于 tke-installer 安装器提供了一个向导式的图形化安装指引界面。 在您 Installer 节点的终端，执行如下脚本： # amd64 arch=amd64 version=v1.3.1 && wget https://tke-release-1251707795.cos.ap-guangzhou.myqcloud.com/tke-installer-linux-$arch-$version.run{,.sha256} && sha256sum --check --status tke-installer-linux-$arch-$version.run.sha256 && chmod +x tke-installer-linux-$arch-$version.run && ./tke-installer-linux-$arch-$version.run # arm64 arch=arm64 version=v1.3.1 && wget https://tke-release-1251707795.cos.ap-guangzhou.myqcloud.com/tke-installer-linux-$arch-$version.run{,.sha256} && sha256sum --check --status tke-installer-linux-$arch-$version.run.sha256 && chmod +x tke-installer-linux-$arch-$version.run && ./tke-installer-linux-$arch-$version.run 您可以查看 TKEStack Release 按需选择版本进行安装，建议您安装最新版本。 tke-installer 约为 7GB，包含安装所需的所有资源。 以上脚本执行完之后，终端会提示访问 http://[tke-installer-IP]:8080/index.html，使用本地主机的浏览器访问该地址，按照指引开始安装控制台，可参考下面的控制台安装。 注意：这里tke-installer-IP地址默认为内网地址，如果本地主机不在集群内网，tke-installer-IP为内网地址所对应的外网地址。 3. 控制台安装 注意：控制台是运行在global集群之上，控制台安装就是在安装global集群。 填写 TKEStack 控制台基本配置信息 用户名：TKEStack 控制台管理员名称（例如：admin） 密码：TKEStack 控制台管理员密码 高可用设置（按需使用，可直接选择【不设置】） TKE提供：在所有 master 节点额外安装 Keepalived 完成 VIP 的配置与连接 使用已有：对接配置好的外部 LB 实例 不设置：访问第一台 master 节点 APIServer 填写 TKEStack 控制台集群设置信息 网卡名称：集群节点使用的网卡，根据实际环境填写正确的网卡名称，默认为eth0（建议使用默认值） GPU 类型：（按需使用，可直接选择【不设置】） 不使用：不安装 Nvidia GPU 相关驱动 Virtual：平台会自动为集群安装 GPUManager 扩展组件 Physical：平台会自动为集群安装 Nvidia-k8s-device-plugin 容器网络： 将为集群内容器分配在容器网络地址范围内的 IP 地址，您可以自定义三大私有网段作为容器网络， 根据您选择的集群内服务数量的上限，自动分配适当大小的 CIDR 段用于 Kubernetes service；根据您选择 Pod 数量上限/节点，自动为集群内每台服务器分配一个适当大小的网段用于该主机分配 Pod 的 IP 地址（建议使用默认值） CIDR： 集群内 Sevice、 Pod 等资源所在网段 Pod数量上限/节点： 决定分配给每个 Node 的 CIDR 的大小 Service数量上限/集群：决定分配给 Sevice 的 CIDR 大小 master 节点： 输入目标机器信息后单击保存，若保存按钮是灰色，单击网页空白处即可变蓝 访问地址： Master 节点内网 IP，请配置至少 8 Cores & 16G内存 及以上的机型，否则会部署失败 SSH 端口：请确保目标机器安全组开放 SSH 端口和 ICMP 协议，否则无法远程登录和 PING 服务器（建议使用22） 用户名和密码： 均为添加的节点的用户名和密码 可以通过节点下面的【添加机器】蓝色字体增加master节点（按需添加） 高级设置（非必须）：可以自定义 Global 集群的 Docker、kube-apiserver、kube-controller-manager、kube-scheduler、kubelet 运行参数 填写 TKEStack 控制台认证信息。（建议使用TKE提供） 认证方式： TKE提供：使用 TKE 自带的认证方式 OIDC：使用 OIDC 认证方式，详见 OIDC 填写 TKEStack 控制台镜像仓库信息。（建议使用TKE提供） 镜像仓库类型： TKE提供：使用 TKE 自带的镜像仓库 第三方仓库：对接配置好的外部镜像仓库，此时，TKEStack 将不会再安装镜像仓库，而是使用您提供的镜像仓库作为默认镜像仓库服务 业务设置 确认是否开启 TKEStack 控制台业务模块。(建议开启) 确实是否开启平台审计功能，审计模块为平台提供了操作记录,用户可以在平台管理进行查询，需用用户提供ES资源。（按需使用，可不开启） 选择 TKEStack 控制台监控存储类型。（建议使用TKE提供） 监控存储类型： TKE提供：使用 TKE 自带的 Influxdb 作为存储 外部 Influxdb：对接外部的 Influxdb 作为存储 外部 ES：对接外部的 Elasticsearch作为存储 不使用：不使用监控 确认是否开启 TKEStack 控制台，选择开启则需要填写控制台域名及证书。（建议使用默认值） 监控存储类型: 自签名证书：使用 TKE 带有的自签名证书 指定服务器证书：填写已备案域名的服务器证书 确认 TKEStack 控制台所有配置是否正确。 开始安装 TKEStack 控制台，安装成功后界面如下，最下面出现【查看指引】的按钮。 点击【查看指引】，按照指引，在本地主机上添加域名解析，以访问 TKEStack 控制台。 以Linux/MacOS为例: 在/etc/hosts文件中加入以下两行域名解析 【IP】 console.tke.com 【IP】 registry.tke.com 注意：这里域名的【IP】地址默认为内网地址，如果本地主机不在集群内网，域名的IP地址应该填该内网地址所对应的外网地址。 4. 访问控制台 在本地主机的浏览器地址输入 http://console.tke.com ，可访问Global集群的控制台界面，输入控制台安装创建的用户名和密码后即可使用TKEStack。 安装常见问题 安装失败请首先检查硬件和软件需求：installation requirements 可参考FAQ installation获得更多帮助。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:44:19 "},"zh/2-产品快速入门/":{"url":"zh/2-产品快速入门/","title":"产品快速入门","keywords":"","body":"产品快速入门 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:03:20 "},"zh/2-产品快速入门/2-1-快速入门.html":{"url":"zh/2-产品快速入门/2-1-快速入门.html","title":"快速入门","keywords":"","body":"快速入门 教程介绍 TKEStack 是一款面向私有化环境的开源容器编排引擎。在本教程中，您将了解如何创建 TKEStack 控制台，并使用控制台创建和管理容器集群，在集群内快速、弹性地部署您的服务。 操作步骤 平台安装 参考：installation-procedures 集群 平台安装之后，可在【平台管理】控制台的【集群管理】中看到 global 集群。如下图所示： TKEStack 还可以另外新建独立集群以及导入已有集群实现多集群的管理。 注意：新建独立集群和导入已有集群都属于TKEStack 架构中的业务集群。 新建独立集群 登录 TKEStack，右上角会出现当前登录的用户名，示例为 admin。 切换至【平台管理】控制台。 在“集群管理”页面中，单击【新建独立集群】。如下图所示： 在“新建独立集群”页面，填写集群的基本信息。新建的集群需满足installation requirements的需求，在满足需求之后，TKEStack 的集群添加非常便利。如下图所示,只需填写【集群名称】、【目标机器】、【密码】，其他保持默认即可添加新的集群。 注意：若【保存】按钮是灰色，单击附近空白处即可变蓝 集群名称： 支持中文，小于 60 字符即可 Kubernetes 版本： 选择合适的 kubernetes 版本，各版本特性对比请查看 Supported Versions of the Kubernetes Documentation。（建议使用默认值） 网卡名称： 最长 63 个字符，只能包含小写字母、数字及分隔符(' - ')，且必须以小写字母开头，数字或小写字母结尾。（建议使用默认值 eth0） VIP ：高可用 VIP 地址。（按需使用） GPU：选择是否安装 GPU 相关依赖。（按需使用） pGPU：平台会自动为集群安装 GPUManager 扩展组件 vGPU：平台会自动为集群安装 Nvidia-k8s-device-plugin 容器网络 ：将为集群内容器分配在容器网络地址范围内的 IP 地址，您可以自定义三大私有网段作为容器网络， 根据您选择的集群内服务数量的上限，自动分配适当大小的 CIDR 段用于 kubernetes service；根据您选择 Pod 数量上限/节点，自动为集群内每台云服务器分配一个适当大小的网段用于该主机分配 Pod 的 IP 地址。（建议使用默认值） CIDR： 集群内 Sevice、 Pod 等资源所在网段。 Pod 数量上限/节点： 决定分配给每个 Node 的 CIDR 的大小。 Service 数量上限/集群 ：决定分配给 Sevice 的 CIDR 大小。 目标机器 ： 目标机器：节点的内网地址。（建议: Master&Etcd 节点配置4 核及以上的机型） SSH 端口： 请确保目标机器安全组开放 22 端口和 ICMP 协议，否则无法远程登录和 PING 云服务器。（建议使用默认值 22） 主机 label：给主机设置 Label,可用于指定容器调度。（按需使用） 认证方式：连接目标机器的方式 密码认证： 密码：目标机器密码 密钥认证： 私钥：目标机器秘钥 私钥密码：目标机器私钥密码，可选填 GPU： 使用 GPU 机器需提前安装驱动和 runtime。（按需使用） 输入以上信息后单击【保存】后还可继续添加集群的节点 提交： 集群信息填写完毕后，【提交】按钮变为可提交状态，单击即可提交。 导入已有集群 登录 TKEStack。 切换至【平台管理】控制台。 在“集群管理”页面，单击【导入集群】。如下图所示： 在“导入集群”页面，填写被导入的集群信息。如下图所示： 名称： 被导入集群的名称，最长 60 字符 API Server： 被导入集群的 API server 的域名或 IP 地址，注意域名不能加上 https:// 端口，此处用的是 https 协议，端口应填 443。 CertFile： 输入被导入集群的 cert 文件内容 Token： 输入被导入集群创建时的 token 值 单击最下方 【提交】 按钮 。 创建业务 注：业务可以实现跨集群资源的使用 登录 TKEStack。 在【平台管理】控制台的【业务管理】中，单击 【新建业务】。如下图所示： 在“新建业务”页面，填写业务信息。如下图所示： 业务名称：不能超过 63 个字符，这里以my-business为例 业务成员： 【访问管理】中【用户管理】中的用户，这里以admin例，即这该用户可以访问这个业务。 集群： 【集群管理】中的集群，这里以gobal集群为例 【填写资源限制】可以设置当前业务使用该集群的资源上限（可不限制） 【新增集群】可以添加多个集群，此业务可以使用多个集群的资源（按需添加） 上级业务：支持多级业务管理，按需选择（可不选） 单击最下方 【完成】 按钮即可创建业务。 创建业务下的命名空间 登录 TKEStack。 在【平台管理】控制台的【业务管理】中，单击【业务 id】。如下图所示： 单击【Namespace 列表】。如下图标签 1 所示： 该页面可以更改业务名称、成员、以及业务下集群资源的限制。 单击【新建 Namespace】。如下图所示： 在“新建 Namespace”页面中，填写相关信息。如下图所示： 名称：不能超过 63 个字符，这里以new-ns为例 集群：my-business业务中的集群，这里以global集群为例 资源限制：这里可以限制当前命名空间下各种资源的使用量，可以不设置。 创建业务下的 Deployment 登录 TKEStack，点击【平台管理】选项旁边的切换按钮，可以切换到【业务管理】控制台。 注意：因为当前登录的是 admin 用户，【业务管理】控制台只包含在创建业务中成员包含 admin 的业务，如果切换到【业务管理】控制台没有看见任何业务，请确认【平台管理】中【业务管理】中的相关业务的成员有没有当前用户，如没有，请添加当前用户。 点击左侧导航栏中的【应用管理】，如果当前用户被分配了多个业务，可通过下图中标签 3 的选择框选择合适的业务。 点击【工作负载】，点击下图标签 4 的【Deployment】，此时进入“Deployment”页面，可通过下图中的标签 5 选择 Deployment 的【命名空间】： 单击上图标签 6【新建】，进入“新建 Workload ”页面。根据实际需求，设置 Deployment 参数。这里参数很多，其中必填信息已用红框标识： 工作负载名：输入自定义名称，这里以my-dep为例 描述：给工作负载添加描述，可不填 标签：给工作负载添加标签，通过工作负载名默认生成 命名空间：根据实际需求进行选择 类型：选择【Deployment（可扩展的部署 Pod）】 数据卷（选填）：为容器提供存储，目前支持临时路径、主机路径、云硬盘数据卷、文件存储 NFS、配置文件、PVC，还需挂载到容器的指定路径中。如需指定容器挂载至指定路径时，单击【添加数据卷】 临时目录：主机上的一个临时目录，生命周期和 Pod 一致 主机路径：主机上的真实路径，可以重复使用，不会随 Pod 一起销毁 NFS 盘：挂载外部 NFS 到 Pod，用户需要指定相应 NFS 地址，格式：127.0.0.1:/data ConfigMap：用户选择在业务 Namespace 下的ConfigMap Secret：用户选择在业务 namespace 下的Secret PVC：用户选择在业务 namespace 下的PVC 实例内容器：根据实际需求，为 Deployment 的一个 Pod 设置一个或多个不同的容器。如下图所示： 名称：自定义，这里以my-container为例 镜像：根据实际需求进行选择，这里以nginx为例 镜像版本（Tag）：根据实际需求进行填写，不填默认为latest CPU/内存限制：可根据 Kubernetes 资源限制 进行设置 CPU 和内存的限制范围，提高业务的健壮性（建议使用默认值） GPU 限制：如容器内需要使用 GPU，此处填 GPU 需求 环境变量：用于设置容器内的变量，变量名只能包含大小写字母、数字及下划线，并且不能以数字开头 新增变量：自己设定变量键值对 引用 ConfigMap/Secret：引用已有键值对 高级设置：可设置 “工作目录”、“运行命令”、“运行参数”、“镜像更新策略”、“容器健康检查”和“特权级”等参数。这里介绍一下镜像更新策略。 镜像更新策略：提供以下 3 种策略，请按需选择 若不设置镜像拉取策略，当镜像版本为空或 latest 时，使用 Always 策略，否则使用 IfNotPresent 策略 Always：总是从远程拉取该镜像 IfNotPresent：默认使用本地镜像，若本地无该镜像则远程拉取该镜像 Never：只使用本地镜像，若本地没有该镜像将报异常 实例数量：根据实际需求选择调节方式，设置实例数量。 手动调节：直接设定实例个数 自动调节：根据设定的触发条件自动调节实例个数，目前支持根据 CPU、内存利用率和利用量出入带宽等调节实例个数 显示高级设置 imagePullSecrets：镜像拉取密钥，用于拉取用户的私有镜像，使用私有镜像首先需要新建 Secret 节点调度策略：根据配置的调度规则，将 Pod 调度到预期的节点。 不使用调度策略：k8s 自动调度 自定义调度规则：通过节点的 Label 来实现 强制满足条件：调度期间如果满足亲和性条件则调度到对应 node，如果没有节点满足条件则调度失败。 尽量满足条件：调度期间如果满足亲和性条件则调度到对应 node，如果没有节点满足条件则随机调度到任意节点。 注释（Annotations）：给 deployment 添加相应 Annotation，如用户信息等 网络模式：选择 Pod 网络模式 OverLay（虚拟网络）：基于 IPIP 和 Host Gateway 的 Overlay 网络方案 FloatingIP（浮动 IP）：支持容器、物理机和虚拟机在同一个扁平面中直接通过 IP 进行通信的 Underlay 网络方案。提供了 IP 漂移能力，支持 Pod 重启或迁移时 IP 不变 NAT（端口映射）：Kubernetes 原生 NAT 网络方案 Host（主机网络）：Kubernetes 原生 Host 网络方案 创建 Service（可选）： Service：勾选【启用】按钮，配置负载端口访问 注意：如果不勾选【启用】则不会创建 Service 服务访问方式：选择是【仅在集群内部访问】该负载还是集群外部通过【主机端口访问】该负载 仅在集群内访问：使用 Service 的 ClusterIP 模式，自动分配 Service 网段中的 IP，用于集群内访问。数据库类等服务如 MySQL 可以选择集群内访问，以保证服务网络隔离 主机端口访问：提供一个主机端口映射到容器的访问方式，支持 TCP、UDP、Ingress。可用于业务定制上层 LB 转发到 Node Headless Service：不创建用于集群内访问的 ClusterIP，访问 Service 名称时返回后端 Pods IP 地址，用于适配自有的服务发现机制。解析域名时返回相应 Pod IP 而不是 Cluster IP 端口映射：输入负载要暴露的端口并指定通信协议类型（容器和服务端口建议都使用 80） Session Affinity: 点击【显示高级设置】出现，会话保持，设置会话保持后，会根据请求 IP 把请求转发给这个 IP 之前访问过的 Pod。默认 None，按需使用 单击【创建 Workload】，完成创建。如下图所示： ​ 当“运行/期望 Pod 数量”相等时，即表示 Deployment 下的所有 Pod 已创建完成。 如果在第 5 步中有创建 Service，则可以再【服务】下的【Service】看到与刚刚创建的 Deployment 同名的 Service 删除资源 在本节中，启动了my-business业务下的 Deployment 和 Service 两种资源，此步骤介绍如何清除所有资源。 删除 Deployment 登录 TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 展开【工作负载】下拉项，进入 “Deployment” 管理页面，选择需要删除【Deployment】的业务下相应的【命名空间】，点击要删除的 Deployment 最右边的【更多】，点击【删除】。如下图所示： 在弹出框中单击【确定】，即可删除 Deployment。 删除 Service 登录 TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 展开【服务】下拉项，进入 “Service” 管理页面，选择需要删除【Service】的业务下相应的【命名空间】，点击要删除的 Service 最右边的【删除】。如下图所示： 在弹出框中单击【确定】，即可删除 Service。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 15:50:01 "},"zh/2-产品快速入门/2-2-入门示例/":{"url":"zh/2-产品快速入门/2-2-入门示例/","title":"入门示例","keywords":"","body":"入门示例 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:03:39 "},"zh/2-产品快速入门/2-2-入门示例/创建简单nginx服务.html":{"url":"zh/2-产品快速入门/2-2-入门示例/创建简单nginx服务.html","title":"操作场景","keywords":"","body":"操作场景 本文档旨在帮助大家了解如何快速创建一个容器集群内的 nginx 服务。 前提条件 已部署 TKEStack 控制台。 已创建集群。如没有另外创建集群，可以先使用global集群。如要尝试创建新集群，请参见 创建集群。 操作步骤 创建 Nginx 服务 登录TKEStack 控制台 。 单击左侧导航栏中【集群管理】，进入“集群管理”页面，单击需要创建服务的集群 ID。 进入【工作负载】的【 Deployment 】中，选择【新建】。如下图所示： 在“新建Workload”页面，只需输入下图中红框的参数即可。 注意：服务所在集群的安全组需要放通节点网络及容器网络，同时需要放通30000 - 32768端口，否则可能会出现容器服务无法使用问题。 单击上图中的【创建Workload】，完成创建。如下图所示： 注意：当运行/期望Pod数量一致时，负载完成创建。 如果在第5步中有创建Service，则可以在【服务】下的【Service】看到与刚刚创建的Deployment同名的Service 访问 Nginx 服务 可通过以下两种方式访问 nginx 服务。 通过主机节点端口访问 nginx 服务 在本地主机的浏览器地址栏输入集群任意节点IP:30000 端口，例如10.0.0.1:30000即可访问服务。如果服务创建成功，访问服务时直接进入 nginx 服务器的默认欢迎页。如下图所示： 注意：如果本地主机在集群内网中，输入节点的内网IP地址即可；如果本地主机不在集群内网中，需要输入节点的外网IP地址 通过服务名称访问 nginx 服务 集群内的其他服务或容器可以直接通过服务名称访问。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-20 15:33:35 "},"zh/2-产品快速入门/2-2-入门示例/如何构建Docker镜像.html":{"url":"zh/2-产品快速入门/2-2-入门示例/如何构建Docker镜像.html","title":"如何构建 Docker 镜像","keywords":"","body":"如何构建 Docker 镜像 说明 DockerHub 提供了大量的镜像可用，详情可查看 DockerHub 官网。 Docker 容器的设计宗旨是让用户在相对独立的环境中运行独立的程序。 Docker 容器程序在镜像内程序运行结束后会自动退出。如果要令构建的镜像在服务中持续运行，需要在创建服务页面指定自身持续执行的程序，如：业务主程序，main 函数入口等。 由于企业环境的多样性，并非所有应用都能在 DockerHub 找到对应的镜像来使用。 您可以通过以下教程了解到如何将应用打包成Docker镜像。 Docker 生成镜像目前有两种方式： 通过 Dockerfile 自动构建镜像； 通过容器操作，并执行 Commit 打包生成镜像。 Dockerfile 自动编译生成（推荐使用） 以 Dockerhub 官方提供的 WordPress 为例，转到 github 查看详情 >> 其 Dockfile 源码如下： FROM php:5.6-apache # install the PHP extensions we need RUN apt-get update && apt-get install -y libpng12-dev libjpeg-dev && rm -rf /var/lib/apt/lists/* \\ && docker-php-ext-configure gd --with-png-dir=/usr --with-jpeg-dir=/usr \\ && docker-php-ext-install gd mysqli opcache # set recommended PHP.ini settings # see https://secure.php.net/manual/en/opcache.installation.php RUN { \\ echo 'opcache.memory_consumption=128'; \\ echo 'opcache.interned_strings_buffer=8'; \\ echo 'opcache.max_accelerated_files=4000'; \\ echo 'opcache.revalidate_freq=2'; \\ echo 'opcache.fast_shutdown=1'; \\ echo 'opcache.enable_cli=1'; \\ } > /usr/local/etc/php/conf.d/opcache-recommended.ini RUN a2enmod rewrite expires VOLUME /var/www/html ENV WORDPRESS_VERSION 4.6.1 ENV WORDPRESS_SHA1 027e065d30a64720624a7404a1820e6c6fff1202 RUN set -x \\ && curl -o wordpress.tar.gz -fSL \"https://wordpress.org/wordpress-${WORDPRESS_VERSION}.tar.gz\" \\ && echo \"$WORDPRESS_SHA1 *wordpress.tar.gz\" | sha1sum -c - \\ # upstream tarballs include ./wordpress/ so this gives us /usr/src/wordpress && tar -xzf wordpress.tar.gz -C /usr/src/ \\ && rm wordpress.tar.gz \\ && chown -R www-data:www-data /usr/src/wordpress COPY docker-entrypoint.sh /usr/local/bin/ RUN ln -s usr/local/bin/docker-entrypoint.sh /entrypoint.sh # backwards compat # ENTRYPOINT resets CMD ENTRYPOINT [\"docker-entrypoint.sh\"] CMD [\"apache2-foreground\"] 通过上述 Dockerfile 文件可以了解到，内置执行了许多的 Linux 命令来安装和部署软件。 操作步骤 在终端创建一个名为worldpress的文件夹，在该文件夹下创建 Dockerfile 文件，文件内容即以上代码。通过 docker build ./命令来构建镜像。 [root@VM_1_98_centos ~]# mkdir worldpress [root@VM_1_98_centos ~]# ls worldpress [root@VM_1_98_centos ~]# cd worldpress/ [root@VM_1_98_centos worldpress]# vi Dockerfile [root@VM_1_98_centos worldpress]# ls Dockerfile [root@VM_1_98_centos worldpress]# docker build ./ Sending build context to Docker daemon 3.072kB Step 1/12 : FROM php:5.6-apache 5.6-apache: Pulling from library/php 5e6ec7f28fb7: Pull complete cf165947b5b7: Pull complete 7bd37682846d: Pull complete ······ 通过 docker images 命令即可查看到构建完成的镜像。 [root@VM_88_88_centos worldpress]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE worldpress latest 9f0b470b5ddb 12 minutes ago 420 MB docker.io/php 5.6-apache eb8333e24502 5 days ago 389.7 MB 使用 Dockerfile 来构建镜像有以下建议： 尽量精简，不安装多余的软件包。 尽量选择 Docker 官方提供镜像作为基础版本，减少镜像体积。 Dockerfile 开头几行的指令应当固定下来，不建议频繁更改，有效利用缓存。 多条 RUN 命令使用'\\'连接，有利于理解且方便维护。 通过 -t 标记构建镜像，有利于管理新创建的镜像。 不在 Dockerfile 中映射公有端口。 Push 前先在本地运行，确保构建的镜像无误。 执行 Commit 实现打包生成镜像 通过 Dockerfile 可以快速构建镜像，而通过 commit 生成镜像可以解决应用在部署过程中有大量交互内容以及难以通过 Dockerfile 构建的问题。 通过 commit 构建镜像操作如下： 运行基础镜像容器，并进入console。[root@VM_88_88_centos ~]# docker run -i -t centos [root@f5f1beda4075 /]# 安装需要的软件，并添加配置。 [root@f5f1beda4075 /]# yum update && yum install openssh-server Loaded plugins: fastestmirror, ovl base | 3.6 kB 00:00:00 extras | 3.4 kB 00:00:00 updates | 3.4 kB 00:00:00 (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 (2/4): extras/7/x86_64/primary_db | 166 kB 00:00:00 (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:00 (4/4): updates/7/x86_64/primary_db ...... ...... ...... Dependency Installed: fipscheck.x86_64 0:1.4.1-5.el7 fipscheck-lib.x86_64 0:1.4.1-5.el7 openssh.x86_64 0:6.6.1p1-25.el7_2 tcp_wrappers-libs.x86_64 0:7.6-77.el7 Complete! 配置完成后打开新终端保存该镜像。 shell [root@VM_88_88_centos ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f5f1beda4075 centos \"/bin/bash\" 8 minutes ago Up 8 minutes hungry_kare [root@VM_88_88_centos ~]# docker commit f5f1beda4075 test:v1.0 sha256:65325ffd2af9d574afca917a8ce81cf8a710e6d1067ee611a87087e1aa88e4a4 [root@VM_88_88_centos ~]# [root@VM_88_88_centos ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE test v1.0 65325ffd2af9 11 seconds ago 307.8 MB TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-20 15:33:35 "},"zh/2-产品快速入门/2-2-入门示例/编写Hello-World程序.html":{"url":"zh/2-产品快速入门/2-2-入门示例/编写Hello-World程序.html","title":"编写 Hello World 程序","keywords":"","body":"编写 Hello World 程序 操作场景 本文档旨在帮助大家了解如何快速创建一个容器集群内的 Hello World 的 Node.js 版的服务。 前提条件 已部署 TKEStack 控制台。 已创建集群。如没有另外创建集群，可以先使用global集群。如要尝试创建新集群，请参见 创建集群。 操作步骤 编写代码制作镜像 编写应用程序 以CentOS 7.6为例 安装node.js，然后依次执行以下命令，创建并进入 hellonode 的文件夹。yum install -y nodejs mkdir hellonode cd hellonode/ 执行以下命令，新建并打开 server.js 文件。vim server.js 按 “i” 或 “insert” 切换至编辑模式，将以下内容输入 server.js。var http = require('http'); var handleRequest = function(request, response) { console.log('Received request for URL: ' + request.url); response.writeHead(200); response.end('Hello World!'); }; var www = http.createServer(handleRequest); www.listen(80); 按 “Esc”，输入 “:wq”，保存文件并返回。 执行以下命令，执行 server.js 文件。node server.js 测试 Hello World 程序，有以下两种办法。 另起一个终端，再次登录节点，执行以下命令。 curl 127.0.0.1:80 # 终端会输出一下信息 Hello World! 打开本地主机的浏览器，以IP地址:端口的形式访问，端口为80。 网页出现Hello world!说明 Hello World 程序运行成功。 注意：如果本地主机不在该节点所在的内网，IP地址应该是该节点的外网地址 创建 Docker 镜像 在 hellonode 文件夹下，创建 Dockerfile 文件。[root@VM_1_98_centos hellonode]# vim Dockerfile 按 “i” 或 “insert” 切换至编辑模式，将以下内容输入 Dockerfile 文件。FROM node:4.4 EXPOSE 80 COPY server.js . CMD node server.js 按 “Esc”，输入 “:wq”，保存文件并返回。 执行以下命令，构建镜像。docker build -t hello-node:v1 . 执行以下命令，查看构建好的 hello-node 镜像。docker images 显示结果如下，则说明 hello-node 镜像已成功构建，记录其 IMAGE ID。如下图所示： 上传该镜像到镜像仓库 已在【组织资源】中的【镜像仓库管理】创建命名空间。 已在【组织资源】中的【访问凭证】创建访问凭证。 依次执行以下命令，上传镜像到 qcloud 仓库。 sudo docker login -u tkestack -p 【访问凭证】 default.registry.tke.com sudo docker tag 【IMAGEID】 default.registry.tke.com/【命名空间】/helloworld:v1 sudo docker push default.registry.tke.com/【命名空间】/helloworld:v1 请将命令中的 【访问凭证】 替换为 已创建的访问凭证。 请将命令中的 【IMAGEID】 替换为 你自己创建镜像的ID，示例中的ID如上图158204134510。 请将命令中的 【命名空间】 替换为 已创建的命名空间。 显示以下结果，则说明镜像上传成功。 在镜像仓库命名空间中进行确认 通过该镜像创建 Hello World 服务 登录 TKEStack 控制台。 单击左侧导航栏中【集群管理】，进入“集群管理”页面。 单击需要创建服务的集群 ID，进入工作负载 “Deployment” 详情页，选择【新建】。如下图所示： 在“新建Workload”页面，仅输入以下红框内容即可： 注意： 镜像，地址要填全：default.registry.tke.com/【命名空间】/【镜像名】，例如：default.registry.tke.com/test/helloworld 服务所在集群的安全组需要放通节点网络及容器网络，同时需要放通30000 - 32768端口，否则可能会出现容器服务无法使用问题。 单击【创建Workload】，完成 Hello World 服务的创建。 访问 Hello World 服务 可通过以下两种方式访问 Hello World 服务。 通过主机节点端口访问 Hello World 服务 选择【服务】>【Service】，在“Service”管理页面，看到与名为helloworld的Deployment同名的 helloworld Service已经运行，如下图所示： 在本地主机的浏览器地址栏输入集群任意节点IP:30000 端口，例如10.0.0.1:30000即可访问服务。如果服务创建成功，访问服务时页面会返回Hello World！ 注意：如果本地主机在集群内网中，输入节点的内网IP地址即可；如果本地主机不在集群内网中，需要输入节点的外网IP地址 通过服务名称访问 Hello World 服务 集群内的其他服务或容器可以直接通过服务名称访问。 更多关于Docker 镜像请参见 如何构建 Docker 镜像 。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-20 15:33:35 "},"zh/3-产品使用指南/":{"url":"zh/3-产品使用指南/","title":"产品使用指南","keywords":"","body":"产品使用指南 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:05:01 "},"zh/3-产品使用指南/3-1-平台侧/":{"url":"zh/3-产品使用指南/3-1-平台侧/","title":"平台侧","keywords":"","body":"平台侧 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:05:20 "},"zh/3-产品使用指南/3-1-平台侧/3-1-1-概览.html":{"url":"zh/3-产品使用指南/3-1-平台侧/3-1-1-概览.html","title":"概览","keywords":"","body":"概览 平台概览页面，可查看TKEStack控制台管理资源的概览。 如下图所示，在【平台管理】页面点击【概览】，此处可以展现： 平台的资源概览 集群的资源状态 快速入口 实用提示 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 15:57:34 "},"zh/3-产品使用指南/3-1-平台侧/3-1-2-集群管理.html":{"url":"zh/3-产品使用指南/3-1-平台侧/3-1-2-集群管理.html","title":"集群管理","keywords":"","body":"集群管理 概念 在这里可以管理你的 Kubernetes 集群。 注意：由于【平台管理】控制台下对集群的大多数操作与【业务管理】控制台完全一致，因此除了集群的【基本信息】和【节点管理】之外，其他栏目（包括命名空间、负载、服务、配置、存储、日志、事件）请参考【业务管理】控制台下相应部分 操作步骤 平台安装之后，可在【平台管理】控制台的【集群管理】中看到global集群。如下图所示： TKEStack还可以另外新建独立集群以及导入已有集群实现多集群的管理。 注意：新建独立集群和导入已有集群都属于TKEStack架构中的业务集群。 新建独立集群 登录 TKEStack，右上角会出现当前登录的用户名，示例为admin。 切换至【平台管理】控制台。 在“集群管理”页面中，单击【新建独立集群】。如下图所示： 在“新建独立集群”页面，填写集群的基本信息。新建的集群需满足installation requirements的需求，在满足需求之后，TKEStack的集群添加非常便利。如下图所示，只需填写【集群名称】、【目标机器】、【密码】，其他保持默认即可添加新的集群。 注意：若【保存】按钮是灰色，单击附近空白处即可变蓝 集群名称： 支持中文，小于60字符即可 Kubernetes版本： 选择合适的kubernetes版本，各版本特性对比请查看 Supported Versions of the Kubernetes Documentation。（建议使用默认值） 网卡名称： 最长63个字符，只能包含小写字母、数字及分隔符(' - ')，且必须以小写字母开头，数字或小写字母结尾。（建议使用默认值eth0） VIP ：高可用 VIP 地址。（按需使用） GPU：选择是否安装 GPU 相关依赖。（按需使用） pGPU：平台会自动为集群安装 GPUManager 扩展组件 vGPU：平台会自动为集群安装 Nvidia-k8s-device-plugin 容器网络 ：将为集群内容器分配在容器网络地址范围内的 IP 地址，您可以自定义三大私有网段作为容器网络， 根据您选择的集群内服务数量的上限，自动分配适当大小的 CIDR 段用于 kubernetes service；根据您选择 Pod 数量上限/节点，自动为集群内每台云服务器分配一个适当大小的网段用于该主机分配 Pod 的 IP 地址。（建议使用默认值） CIDR： 集群内 Sevice、 Pod 等资源所在网段。 Pod数量上限/节点： 决定分配给每个 Node 的 CIDR 的大小。 Service数量上限/集群 ：决定分配给 Sevice 的 CIDR 大小。 目标机器 ： 目标机器：节点的内网地址。（建议: Master&Etcd 节点配置4核及以上的机型） SSH端口： 请确保目标机器安全组开放 22 端口和 ICMP 协议，否则无法远程登录和 PING 云服务器。（建议使用默认值22） 主机label：给主机设置Label,可用于指定容器调度。（按需使用） 认证方式：连接目标机器的方式 密码认证： 密码：目标机器密码 密钥认证： 私钥：目标机器秘钥 私钥密码：目标机器私钥密码，可选填 GPU： 使用GPU机器需提前安装驱动和runtime。（按需使用） 输入以上信息后单击【保存】后还可继续添加集群的节点 提交： 集群信息填写完毕后，【提交】按钮变为可提交状态，单击即可提交。 导入已有集群 登录 TKEStack。 切换至【平台管理】控制台。 在“集群管理”页面，单击【导入集群】。如下图所示： 在“导入集群”页面，填写被导入的集群信息。如下图所示： 名称： 被导入集群的名称，最长60字符 API Server： 被导入集群的API server的域名或IP地址 CertFile： 输入被导入集群的cert 文件内容 Token： 输入被导入集群创建时的token值 单击最下方 【提交】 按钮 。 对集群的操作 基本信息 登录 TKEStack。 切换至【平台管理】控制台。 在“集群管理”页面中，点击要操作的集群ID，如下图“global”所示。 点击【基本信息】，可查看集群基本信息 集群名称 集群ID 状态 Kubernetes版本 网卡名称 容器网络 集群凭证：在本地主机安装kubectl后，可用过用户名密码或集群CA证书登陆到集群。 超售比：Kubernetes对象在申请资源时，如果申请总额大于硬件配置，则无法申请，但是很多时候部分对象并没有占用这么多资源，因此可以设置超售比来扩大资源申请总额，以提高硬件资源利用率。可通过超售比最后面的笔的符号对超售比更改 创建时间 节点管理 登录 TKEStack。 切换至【平台管理】控制台。 在“集群管理”页面中，点击要操作的集群ID，如下图“global”所示。 点击【节点管理】中的【节点】，可查看当前集群的“节点列表” 点击蓝色【添加节点】按钮可增加当前集群的Worker节点 目标机器：建议内网地址，要求添加的节点和当前集群的其他机器在同一内网。 SSH端口：默认22 主机label：按需添加 认证方式：按需选择 用户名：默认root 密码：目标机器用户名为root的密码 GPU：按需选择 点击蓝色【监控】按钮可监控节点，可以从Pod、CPU、内存、硬盘、网络等维度监控 对节点的可以的操作如下图所示： 移出：仅针对worker节点，将节点移出集群 驱逐：节点驱逐后，将会把节点内的所有Pod（不包含DaemonSet管理的Pod）从节点中驱逐到集群内其他节点，并将节点设置为封锁状态。 注意：本地存储的Pod被驱逐后数据将丢失，请谨慎操作 编辑标签：编辑节点标签 编辑Taint：编辑Taint后，新的pod不能被调度到该节点，但可以通过编辑pod的toleration使其可以调度到具有匹配的taint节点上。 封锁：封锁节点后，将不接受新的Pod调度到该节点，需要手动取消封锁的节点。 点击其中一个节点名，例如上图中的【172.19.0.154】，可以看到该节点更多的信息，具体包括： Pod管理：可查看当前节点下的pod 销毁重建：销毁该pod，重新新建该pod 远程登录：登录到该pod的shell里 事件：关于该节点上资源的事件 详情：包括节点主机信息以及Kubernetes信息 YAML：节点的YAML文件 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 15:58:13 "},"zh/3-产品使用指南/3-1-平台侧/3-1-3-业务管理.html":{"url":"zh/3-产品使用指南/3-1-平台侧/3-1-3-业务管理.html","title":"业务管理","keywords":"","body":"业务管理 概念 在这里用户可以管理线上业务。 操作步骤 新建业务 注：业务可以实现跨集群资源的使用 登录 TKEStack。 在【平台管理】控制台的【业务管理】中，单击 【新建业务】。如下图所示： 在“新建业务”页面，填写业务信息。如下图所示： 业务名称：不能超过63个字符，这里以my-business为例 业务成员： 【访问管理】中【用户管理】中的用户，这里以admin例，即该用户可以访问这个业务。 集群： 【集群管理】中的集群，这里以gobal集群为例 【填写资源限制】可以设置当前业务使用该集群的资源上限（可不限制） 【新增集群】可以添加多个集群，此业务可以使用多个集群的资源（按需添加） 上级业务：支持多级业务管理，按需选择（可不选） 单击最下方 【完成】 按钮即可创建业务。 添加业务成员 登录 TKEStack。 切换至 【平台管理】控制台，点击【业务管理】。 在“业务管理”页面中，可以看到已创建的业务列表。鼠标移动到要修改的业务上(无需点击)，成员列会出现修改图标按钮。如下图所示： 注意：修改业务成员仅限状态为Active的业务，这里可以新建和删除成员。 查看业务监控 登录 TKEStack。 切换至 【管理】控制台，点击【业务管理】。 在“业务管理”页面中，可以看到已创建的业务列表。点击监控按钮，如下图所示： 在右侧弹出窗口里查看业务监控情况，如下图所示： 删除业务 登录 TKEStack。 切换至 【平台管理】控制台，点击【业务管理】。 在“业务管理”页面中，可以看到已创建的业务列表。点击删除按钮，如下图所示： 注意：删除业务成员仅限状态为Active的业务 对业务的操作 登录 TKEStack。 在【平台管理】控制台的【业务管理】中，单击【业务id】。如下图所示： a. 业务信息： 在这里可以对业务名称、关联的集群、关联集群的资源进行限制等操作。 b. 成员列表： 在这里可以对业务名称、关联的集群、关联集群的资源进行限制等操作。 c. 子业务： 在这里可以新建本业务的子业务或通过导入子业务将已有业务变成本业务的子业务 d. 业务下Namespace列表： 这里可以管理业务下的Namespace ​ 单击【新建Namespace】。在“新建Namespace”页面中，填写相关信息。如下图所示： ​ 名称：不能超过63个字符，这里以new-ns为例 ​ 集群：my-business业务中的集群，这里以global集群为例 ​ 资源限制*：这里可以限制当前命名空间下各种资源的使用量，可以不设置。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 15:58:24 "},"zh/3-产品使用指南/3-1-平台侧/3-1-4-扩展组件.html":{"url":"zh/3-产品使用指南/3-1-平台侧/3-1-4-扩展组件.html","title":"扩展组件","keywords":"","body":"扩展组件 概念 这里用户可以管理集群扩展组件。 操作步骤 创建组件 登录 TKEStack。 切换至 【平台管理】控制台，选择【扩展组件】页面。 选择需要安装组件的集群，点击【新建】按钮。如下图所示： 注意：此页面右边的【删除】按钮可以删除安装了的组件 在弹出的扩展组件列表里，选择要安装的组件。如下图所示： 注意：如果选择的是PersistentEvent，需要在下方输入地址和索引。 单击【完成】。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 15:58:40 "},"zh/3-产品使用指南/3-1-平台侧/3-1-5-组织资源.html":{"url":"zh/3-产品使用指南/3-1-平台侧/3-1-5-组织资源.html","title":"组织资源","keywords":"","body":"组织资源 概念 这里用户可以管理镜像仓库和凭据。 镜像仓库管理 这里用户可以在镜像仓库里创建属于自己的命名空间，管理镜像。 操作步骤 新建命名空间 登录 TKEStack。 切换至【平台管理】控制台，选择 【组织资源】->【镜像仓库管理】。 点击【新建】按钮。如下图所示： 在弹出的“新建命名空间”页面，填写命名空间信息，如下图所示： 名称： 命名空间名字，不超过63字符 描述： 命名空间描述信息（可选） 权限类型： 选择命名空间权限类型 公有： 所有人均可访问该命名空间下的镜像 私有： 个人用户命名空间 单击【确认】按钮删除命名空间 登录 TKEStack。 切换至 【平台管理】控制台，选择 【组织资源】->【镜像仓库管理】。点击列表最右侧【删除】按钮。如下图所示： 镜像上传 登录 TKEStack。 切换至 【平台管理】控制台，选择 【组织资源】->【镜像仓库管理】，查看命名空间列表。点击列表中命名空间【名称】。如下图所示： 此时进入了“镜像列表”页面，点击【镜像上传指引】按钮。如下图所示： 注意：此页面可以通过上传的镜像最右边的【删除】按钮来删除上传的镜像 根据指引内容，在物理节点上执行相应命令。如下图所示： Chart 包仓库管理 这里用户可以在 Chart 包仓库里创建属于自己的 Chart 包命名空间，管理 Chart。 操作步骤 新建 Chart 包命名空间 登录 TKEStack。 切换至 【平台管理】控制台，选择 【组织资源】->【 Chart 包仓库管理】，查看 “Chart 包命名空间”列表 点击【新建】按钮。如下图所示： 在弹出的新建 ChartGroup 页面，填写 ChartGroup 信息，如下图所示： 名称： ChartGroup 名字，不超过63字符 描述： ChartGroup 描述信息（可选） 权限类型： 选择 ChartGroup 权限类型 公有： 所有人均可访问该 ChartGroup 下的Chart 私有： 个人用户C hartGroup 单击【确认】按钮删除 Chart 包命名空间 登录 TKEStack。 切换至 【平台管理】控制台，选择 【组织资源】-> 【Chart 包仓库管理】，查看 “Chart 包命名空间”列表。 点击列表最右侧【删除】按钮。如下图所示： Chart上传 登录 TKEStack。 切换至 【平台管理】控制台，选择 【组织资源】-> 【Chart 包仓库管理】，查看 “Chart 包命名空间”列表。 点击列表中 Chart 包命名空间【名称】。如下图所示： 点击命名空间的【Chart上传指引】按钮。如下图所示： 根据指引内容，在物理节点上执行相应命令。如下图所示： 访问凭证 这里用户可以管理自己的凭据 操作步骤 新建访问凭证 登录 TKEStack。 切换至 【平台管理】控制台，选择 【组织资源】下的【访问凭证】，点击【新建】按钮。如下图所示： 在弹出创建访问凭证页面，填写凭证信息。如下图所示： 凭证描述： 描述当前凭证信息 过期时间： 填写过期时间，选择小时/分钟为单位 单击【确认】按钮 停用/启用/删除访问凭证 登录 TKEStack。 切换至 【平台管理】控制台，选择 【组织资源】-> 【访问凭证】，查看“访问凭证”列表。单击列表右侧【禁用】/【启用】/【删除】按钮。如下图所示： 注意：点击【禁用】之后，【禁用】按钮就变成了【启用】 单击【确认】按钮 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:00:16 "},"zh/3-产品使用指南/3-1-平台侧/3-1-6-访问管理.html":{"url":"zh/3-产品使用指南/3-1-平台侧/3-1-6-访问管理.html","title":"访问管理","keywords":"","body":"访问管理 概念 这里用户可以管理用户和策略 用户管理 用户 新建用户 登录TKEStack。 切换至【平台管理】控制台，选择【访问管理】->【用户管理】。 点击【新建】按钮。如下图所示： 在弹出的添加用户窗口填写用户信息。如下图所示： 用户账号： 长度3～32位字符，小写字母或数字开头结尾，中间包含小写字母、数字、- 用户名称： 长度需小于256字符，用户名称会显示在页面右上角 用户密码： 10~16位字符，需包括大小写字母及数字 确认密码： 再次输入密码。 手机号： 输入用户手机号 邮箱： 输入用户邮箱 平台角色： 管理员： 平台预设角色，允许访问和管理所有平台和业务的功能和资源 平台用户： 平台预设角色，允许访问和管理大部分平台功能，可以新建集群及业务 租户： 平台预设角色，不绑定任何平台权限，仅能登录 自定义： 通过勾选下面的策略给用户自定义独立的权限 单击【保存】按钮。 修改密码 登录TKEStack。 切换至【平台管理】控制台，选择【访问管理】->【用户管理】，查看用户列表。 点击用户列表最右侧的【修改密码】按钮。如下图所示： 注意：此页面也可以通过【修改密码】旁边的【删除】来删除用户 在弹出的修改密码窗口里输入新的密码并确认。如下图所示： 单击【保存】按钮。 编辑用户基本信息 登录TKEStack 切换至【平台管理】控制台，选择【访问管理】->【用户管理】，查看用户列表。 点击列表中的用户名称。如下图所示： 在用户基本信息页面，单击 基本信息 旁的【编辑】按钮。如下图所示： 在弹出的用户信息框内编辑用户信息。 单击【保存】按钮。 用户组 新建用户组 登录TKEStack 切换至【平台管理】控制台，选择【访问管理】->【用户管理】-> 【用户组】。 点击【新建】按钮。如下图所示： 在弹出的添加用户窗口填写用户信息。如下图所示： 用户组名称： 长度需小于60位字符，小写字母或数字开头结尾，中间包含小写字母、数字、- 用户组描述： 长度需小于255字符 关联用户： 点按用户ID/名称前面的方框可以关联相应的用户，支持全选和按住shift键多选 平台角色： 管理员： 平台预设角色，允许访问和管理所有平台和业务的功能和资源 平台用户： 平台预设角色，允许访问和管理大部分平台功能，可以新建集群及业务 租户： 平台预设角色，不绑定任何平台权限，仅能登录 自定义： 通过勾选下面的策略给用户自定义独立的权限 单击【提交】按钮。编辑用户组基本信息 登录TKEStack 切换至【平台管理】控制台，选择【访问管理】->【用户组管理】，查看用户组列表。 点击列表中的用户组名称。如下图1所示： 此界面也可以更改关联用户，如上图中的2所示，和 新建用户组 ->添加用户组 中一样的步骤来关联用户。 在用户组基本信息页面，单击 基本信息 旁的【编辑】按钮。如下图1所示： 在弹出的信息框内可以编辑 用户组名称 和 用户组描述，此时会出现【提交】按钮，点击后可更改用户组基本信息。 如上图中的2所示，此界面也可以更改关联用户，点击蓝色【关联用户】按钮后，和 新建用户组 ->添加用户组 中一样的操作来关联用户。这里还可以点击查看【已关联角色】和【已关联策略】 删除用户组 登录TKEStack 切换至【平台管理】控制台，选择【访问管理】->【用户组管理】，查看用户组列表。 点击用户组列表最右侧的【删除】按钮。如下图所示： 在弹出的确认删除窗口，单击【确认】。 策略管理 平台策略 新建策略 登录TKEStack 切换至【平台管理】控制台，选择【访问管理】->【策略管理】 点击【新建】按钮。如下图所示： 在弹出的新建策略窗口输入策略信息。如下图所示： 策略名称： 长度需要小于256个字符 效果： 策略动作，允许/拒绝 服务： 选择策略应用于哪项服务 操作： 选择对应服务的各项操作权限 资源： 输入资源label，支持模糊匹配，策略将应用于匹配到的资源 描述： 输入策略描述 单击【保存】按钮。关联用户和用户组 登录TKEStack。 切换至【平台管理】控制台，选择【访问管理】->【策略管理】，查看策略列表。 点击列表中最右侧【关联用户】或【关联用户组】按钮。如下图所示： 此界面也可以点击最右边的【删除】按钮来删除策略 在弹出的关联用户窗口选择用户或用户组，这里以用户组为例。如下图所示： 单击【确定】按钮 编辑策略基本信息 登录TKEStack 切换至【平台管理】控制台，选择【访问管理】->【策略管理】，查看策略列表。 点击列表中的策略名称。如下图所示： 在策略基本信息页面，单击 基本信息 旁的【编辑】按钮。如下图所示： 在弹出的信息框内编辑策略名称和描述。 单击【保存】按钮。 业务策略 新建策略 登录TKEStack 切换至【平台管理】控制台，选择【访问管理】->【策略管理】->【业务策略】 点击【新建】按钮。如下图所示： 在弹出的新建策略窗口输入策略信息。如下图所示： 策略名称： 长度需要小于256个字符 效果： 策略动作，允许/拒绝 服务： 选择策略应用于哪项服务 操作： 选择对应服务的各项操作权限 资源： 输入资源label，支持模糊匹配，策略将应用于匹配到的资源 描述： 输入策略描述 单击【保存】按钮。 编辑策略基本信息 登录TKEStack 切换至【平台管理】控制台，选择【访问管理】->【策略管理】，查看策略列表。 点击列表中的策略名称。如下图所示： 在策略基本信息页面，单击 基本信息 旁的【编辑】按钮。如下图所示： 在弹出的信息框内编辑策略名称和描述。 单击【保存】按钮。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:00:03 "},"zh/3-产品使用指南/3-1-平台侧/3-1-7-监控&告警.html":{"url":"zh/3-产品使用指南/3-1-平台侧/3-1-7-监控&告警.html","title":"监控和告警","keywords":"","body":"监控和告警 概念 这里用户可以管理设置平台告警和消息通知。 告警设置 概念 这里用户配置平台告警。 前提条件 需要设置告警的集群应该先在【扩展组件】安装Prometheus组件 操作步骤 新建告警设置 登录 TKEStack。 切换至【平台管理】控制台，选择 【监控&告警】下的【告警设置】，查看“告警设置”列表。 选择相应【集群】，点击【新建】按钮。如下图所示： 在“新建策略”页面填写告警策略信息。如下图所示： 告警策略名称： 输入告警策略名称，最长60字符 策略类型： 选择告警策略应用类型 集群： 集群监控告警 Pod： Pod 监控告警 告警对象： 选择Pod相关的告警对象，支持对namespace下不同对deployment、stateful和daemonset 进行监控报警 按工作负载选择： 选择namespace下的某个工作负载 全部选择： 不区分namespace，全部监控 节点： 节点监控告警 统计周期： 选择数据采集周期，支持1、2、3、4、5分钟 指标： 选择告警指标，支持对监测值与指标值进行【大于/小于】比较，选择结果持续周期。如下图： 接收组： 选择接收组，当出现满足条件当报警信息时，向组内人员发送消息。接收组需要先在 通知设置 创建 通知方式： 选择通知渠道和消息模版。通知渠道 和 消息模版需要先在 通知设置 创建 添加通知方式 如需要添加多种通知方式，点击该按钮。 单击【提交】按钮。复制告警设置 登录 TKEStack。 切换至【平台管理】控制台，选择 【监控&告警】下的【告警设置】，查看“告警设置”列表。 选择相应【集群】，点击告警设置列表最右侧的【复制】按钮。如下图所示： 在“复制策略”页面，编辑告警策略信息。 单击【提交】按钮。编辑告警设置 登录TKEStack 切换至【平台管理】控制台，选择 【监控&告警】下的【告警设置】，查看“告警设置”列表。 选择相应【集群】，点击【告警名称】。如下图所示： 在“告警策略详情”页面，单击【基本信息】右侧的【编辑】按钮。如下图所示： 在“更新策略”页面，编辑策略信息。 单击【提交】按钮。删除告警设置 登录 TKEStack。 切换至【平台管理】控制台，选择 【监控&告警】下的【告警设置】，查看“告警设置”列表。 选择相应【集群】，点击列表最右侧的【删除】按钮。如下图所示： 在弹出的删除告警窗口，单击【确定】按钮。批量删除告警设置 登录TKEStack 切换至【平台管理】控制台，选择 【监控&告警】下的【告警设置】，查看“告警设置”列表。 选择相应【集群】，选择多个告警策略，单击告警设置下方的【删除】按钮。如下图所示： 在弹出的删除告警窗口，单击【确定】按钮。 通知设置 概念 这里用户配置平台通知。 操作步骤 通知渠道 新建通知渠道 登录 TKEStack。 切换至【平台管理】控制台，选择 【监控&告警】->【通知设置】->【通知渠道】，查看“通知渠道”列表。 点击【新建】按钮。如下图所示： 在“新建通知渠道”页面填写渠道信息。如下图所示： 名称： 填写渠道名称 渠道： 选择渠道类型，输入渠道信息 邮件： 邮件类型 email： 邮件发送放地址 password： 邮件发送方密码 smtpHost： smtp IP地址 smtpPort： smtp端口 tls： 是否tls加密 短信： 短信方式 appKey： 短信发送方的appKey sdkAppID： sdkAppID extend： extend 信息 微信公众号： 微信公众号方式 appID： 微信公众号appID appSecret： 微信公众号app密钥 单击【保存】按钮。 编辑通知渠道 登录 TKEStack。 切换至【平台管理】控制台，选择 【监控&告警】->【通知设置】->【通知渠道】，查看“通知渠道”列表。 单击渠道名称。如下图所示： 在“基本信息”页面，单击【基本信息】右侧的【编辑】按钮。如下图所示： 在“更新渠道通知”页面，编辑渠道信息。 单击【保存】按钮。 删除通知渠道 登录 TKEStack。 切换至【平台管理】控制台，选择 【监控&告警】->【通知设置】->【通知渠道】，查看“通知渠道”列表。 选择要删除的渠道，点击【删除】按钮。如下图所示： 单击删除窗口的【确定】按钮。 通知模板 新建通知模版 登录 TKEStack。 切换至【平台管理】控制台，选择 【监控&告警】->【通知设置】->【通知模板】，查看“通知模板”列表。 点击【新建】按钮。如下图所示： 在“新建通知模版”页面填写模版信息。如下图所示： 名称： 模版名称 渠道： 选择已创建的渠道 body： 填写消息body体 header： 填写消息标题 单击【保存】按钮。 编辑通知模版 登录 TKEStack。 切换至【平台管理】控制台，选择 【监控&告警】->【通知设置】->【通知模板】，查看“通知模板”列表。 单击模版名称。如下图所示： 在基本信息页面，单击【基本信息】右侧的【编辑】按钮。如下图所示： 在“更新通知模版”页面，编辑模版信息。 单击【保存】按钮。 删除通知模版 登录 TKEStack。 切换至【平台管理】控制台，选择 【监控&告警】->【通知设置】->【通知模板】，查看\"通知模板\"列表。 选择要删除的模版，点击【删除】按钮。如下图所示： 单击删除窗口的【确定】按钮。 接收人 新建接收人 登录 TKEStack。 切换至【平台管理】控制台，选择 【监控&告警】->【通知设置】->【接收人】，查看\"接收人\"列表。 点击【新建】按钮。如下图所示： 在“新建接收人”页面填写模版信息。如下图所示： 显示名称： 接收人显示名称 用户名： 接收人用户名 移动电话： 手机号 电子邮件： 接收人邮箱 微信OpenID： 接收人微信ID 单击【保存】按钮。 编辑接收人信息 登录 TKEStack。 切换至【平台管理】控制台，选择 【监控&告警】->【通知设置】->【接收人】，查看“接收人”列表。 单击接收人名称。如下图所示： 在“基本信息”页面，单击【基本信息】右侧的【编辑】按钮。如下图所示： 在“更新接收人”页面，编辑接收人信息。 单击【保存】按钮。 删除接收人 登录 TKEStack。 切换至【平台管理】控制台，选择 【监控&告警】->【通知设置】->【接收人】，查看“接收人”列表。 选择要删除的接收人，点击【删除】按钮。如下图所示： 单击删除窗口的【确定】按钮。 接收组 新建接收组 登录 TKEStack。 切换至【平台管理】控制台，选择 【监控&告警】->【通知设置】->【接收组】，查看“接收组”列表。 点击【新建】按钮。如下图所示： 在“新建接收组”页面填写模版信息。如下图所示： 名称： 接收组显示名称 接收组： 从列表里选择接收人。如没有想要的接收人，请在接收人里创建。 单击【保存】按钮。 编辑接收组信息 登录 TKEStack。 切换至【平台管理】控制台，选择 【监控&告警】->【通知设置】->【接收组】，查看“接收组”列表。 单击接收组名称。如下图所示： 在“基本信息”页面，单击【基本信息】右侧的【编辑】按钮。如下图所示： 在“更新接收组”页面，编辑接收组信息。 单击【保存】按钮。 删除接收组 登录 TKEStack。 切换至【平台管理】控制台，选择 【监控&告警】->【通知设置】->【接收组】，查看“接收组”列表。 选择要删除的接收组，点击【删除】按钮。如下图所示： 单击删除窗口的【确定】按钮。 告警记录 概念 这里可以查看历史告警记录” 操作步骤 登录 TKEStack。 切换至【平台管理】控制台，选择 【监控&告警】->【告警记录】查看“历史告警记录”列表。如下图所示： TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:00:10 "},"zh/3-产品使用指南/3-1-平台侧/3-1-8-运维中心.html":{"url":"zh/3-产品使用指南/3-1-平台侧/3-1-8-运维中心.html","title":"运维中心","keywords":"","body":"运维中心 概念 这里用户可以管理 Helm 应用、日志和集群事件持久化。 Helm应用 概念 这里给集群创建 Helm 应用。 前提条件 需要使用Helm应用的集群应该先在【扩展组件】安装Helm组件 操作步骤 新建Helm应用 登录 TKEStack。 切换至【平台管理】控制台，选择【运维中心】->【 Helm 应用】。 选择相应【集群】，单击【新建】按钮。如下图所示： 在“新建Helm应用”页面填写Helm应用信息。如下图所示： 应用名： 输入应用名，1～63字符，只能包含小写字母、数字及分隔符(\"-\")，且必须以小写字母开头，数字或小写字母结尾 Chart_Url： 输入Chart 文件地址 类型： 选择应用类型 公有： 公有类型 私有： 私有类型 用户名： 私有用户名 密码： 私有密码 Key-Value： 通过新增变量替换Chart包中默认配置 单击【完成】按钮。 日志采集 概念 这里用户可以管理各个集群的日志采集 前提：在【平台管理】控制台，【运维中心】->【日志组件】中开启需要采集日志集群的组件，当组件的状态为“运行中”时，表示可以新建日志采集规则。 操作步骤 新建日志采集规则 登录 TKEStack。 切换至【平台管理】控制台，选择 【运维中心】->【日志采集】。 选择相应【集群】和【命名空间】，单击【新建】按钮。如下图所示： 在“新建日志采集”页面填写日志采集信息。如下图所示： 收集规则名称： 输入规则名，1～63字符，只能包含小写字母、数字及分隔符(\"-\")，且必须以小写字母开头，数字或小写字母结尾 所属集群： 选择所属集群 类型： 选择采集类型 容器标准输出： 容器Stdout信息采集 日志源： 可以选择所有容器或者某个namespace下的所有容器/工作负载 所有容器： 所有容器 指定容器： 某个Namespace下的所有容器或者工作负载 容器文件路径： 容器内文件内容采集 日志源： 可以采集具体容器内的某个文件路径下的文件内容 工作负载选项： 选择某个namespace下的某种工作负载类型下的某个工作负载 配置采集路径： 选择某个容器下的某个文件路径 节点文件路径： 收集节点上某个路径下的文件内容+ **日志源：** + **收集路径：** 节点上日志收集路径 + **metadata：** key：value格式，收集的日志会带上metadata信息上报给消费端 消费端： 选择日志消费端 Kafka： 访问地址： kafka ip和端口 主题（Topic）： kafka topic名 Elasticsearch： Elasticsearch地址： ES 地址，如：http://190.0.0.1:200 索引： ES索引，最长60个字符，只能包含小写字母、数字及分隔符(\"-\"、\"_\"、\"+\")，且必须以小写字母开头 单击【完成】按钮。 事件持久化 概念 这里用户可以管理各个集群的事件 操作步骤 设置事件持久化 登录 TKEStack。 切换至【平台管理】控制台，选择 【运维中心】->【事件持久化】，查看事件持久化列表。 单击列表最右侧【设置】按钮。如下图所示： 在“设置事件持久化”页面填写持久化信息。如下图所示： 事件持久化存储： 是否进行持久化存储 Elasticsearch地址： ES 地址，如：http://190.0.0.1:200 索引： ES索引，最长60个字符，只能包含小写字母、数字及分隔符(\"-\"、\"_\"、\"+\")，且必须以小写字母开头 单击【完成】按钮。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:00:23 "},"zh/3-产品使用指南/3-2-业务侧/":{"url":"zh/3-产品使用指南/3-2-业务侧/","title":"业务侧","keywords":"","body":"业务侧 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:05:55 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/","title":"应用管理","keywords":"","body":"应用管理 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:06:16 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-1-命名空间.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-1-命名空间.html","title":"命名空间","keywords":"","body":"简介 Namespaces 是 Kubernetes 在同一个集群中进行逻辑环境划分的对象， 您可以通过 Namespaces 进行管理多个团队多个项目的划分。在 Namespaces 下，Kubernetes 对象的名称必须唯一。您可以通过资源配额进行可用资源的分配，还可以进行不同 Namespaces 网络的访问控制。 使用方法 通过 TKEStack 控制台使用：TKEStack 控制台提供 Namespaces 的增删改查功能。 【业务管理】平台下不支持对命名空间的直接操作，需在【平台管理】下【业务管理】中指定业务通过“创建业务下的命名空间”来实现。 通过 Kubectl 使用：更多详情可查看 Kubernetes 官网文档。 相关知识 通过 ResourceQuota 设置 Namespaces 资源的使用配额 一个命名空间下可以拥有多个 ResourceQuota 资源，每个 ResourceQuota 可以设置每个 Namespace 资源的使用约束。可以设置 Namespaces 资源的使用约束如下： 计算资源的配额，例如 CPU、内存。 存储资源的配额，例如请求存储的总存储。 Kubernetes 对象的计数，例如 Deployment 个数配额。 不同的 Kubernetes 版本，ResourceQuota 支持的配额设置略有差异，更多详情可查看 Kubernetes ResourceQuota 官方文档。 ResourceQuota 的示例如下所示： apiVersion: v1 kind: ResourceQuota metadata: name: object-counts namespace: default spec: hard: configmaps: \"10\" ## 最多10个 ConfigMap replicationcontrollers: \"20\" ## 最多20个 replicationcontroller secrets: \"10\" ## 最多10个 secret services: \"10\" ## 最多10个 service services.loadbalancers: \"2\" ## 最多2个 Loadbanlacer 模式的 service cpu: \"1000\" ## 该 Namespaces 下最多使用1000个 CPU 的资源 memory: 200Gi ## 该 Namespaces 下最多使用200Gi的内存 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:03:20 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-2-工作负载/":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-2-工作负载/","title":"工作负载","keywords":"","body":"工作负载 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:06:34 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-2-工作负载/3-2-1-2-1-Development.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-2-工作负载/3-2-1-2-1-Development.html","title":"Development","keywords":"","body":"简介 Deployment 声明了 Pod 的模板和控制 Pod 的运行策略，适用于部署无状态的应用程序。您可以根据业务需求，对 Deployment 中运行的 Pod 的副本数、调度策略、更新策略等进行声明。 Deployment 控制台操作指引 创建 Deployment 登录TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择需要创建Deployment的【业务】下相应的【命名空间】，展开【工作负载】下拉项，进入【Deployment】管理页面。如下图所示： 单击【新建】，进入 “新建Workload” 页面。根据实际需求，设置 Deployment 参数。关键参数信息如下，其中必填项为工作负载名、实例内容器的名称和镜像： 工作负载名：输入自定义名称。 标签：给工作负载添加标签 命名空间：根据实际需求进行选择。 类型：选择【Deployment（可扩展的部署 Pod）】。 数据卷：根据需求，为负载添加数据卷为容器提供存，目前支持临时路径、主机路径、云硬盘数据卷、文件存储NFS、配置文件、PVC，还需挂载到容器的指定路径中 临时目录：主机上的一个临时目录，生命周期和Pod一致 主机路径：主机上的真实路径，可以重复使用，不会随Pod一起销毁 NFS盘：挂载外部NFS到Pod，用户需要指定相应NFS地址，格式：127.0.0.1:/data ConfigMap：用户在业务Namespace下创建的ConfigMap Secret：用户在业务namespace下创建的Secret PVC：用户在业务namespace下创建的PVC 实例内容器：根据实际需求，为 Deployment 的一个 Pod 设置一个或多个不同的容器。 名称：自定义 镜像：根据实际需求进行选择 镜像版本（Tag）：根据实际需求进行填写，不填默认为latest CPU/内存限制：可根据 Kubernetes 资源限制 进行设置 CPU 和内存的限制范围，提高业务的健壮性（建议使用默认值） GPU限制：如容器内需要使用GPU，此处填GPU需求 环境变量：用于设置容器内的变量，变量名只能包含大小写字母、数字及下划线，并且不能以数字开头 新增变量：自己设定变量键值对 引用ConfigMap/Secret：引用已有键值对 高级设置：可设置 “工作目录”、“运行命令”、“运行参数”、“镜像更新策略”、“容器健康检查”和“特权级”等参数。这里介绍一下镜像更新策略。 镜像更新策略：提供以下3种策略，请按需选择 若不设置镜像拉取策略，当镜像版本为空或 latest 时，使用 Always 策略，否则使用 IfNotPresent 策略 Always：总是从远程拉取该镜像 IfNotPresent：默认使用本地镜像，若本地无该镜像则远程拉取该镜像 Never：只使用本地镜像，若本地没有该镜像将报异常 实例数量：根据实际需求选择调节方式，设置实例数量。 手动调节：直接设定实例个数 自动调节：根据设定的触发条件自动调节实例个数，目前支持根据CPU、内存利用率和利用量出入带宽等调节实例个数 定时调节：根据Crontab 语法周期性设置实例个数 imagePullSecrets：镜像拉取密钥，用于拉取用户的私有镜像 节点调度策略：根据配置的调度规则，将Pod调度到预期的节点。支持指定节点调度和条件选择调度 注释（Annotations）：给Deployment添加相应Annotation，如用户信息等 网络模式：选择Pod网络模式 OverLay（虚拟网络）：基于 IPIP 和 Host Gateway 的 Overlay 网络方案 FloatingIP（浮动 IP）：支持容器、物理机和虚拟机在同一个扁平面中直接通过IP进行通信的 Underlay 网络方案。提供了 IP 漂移能力，支持 Pod 重启或迁移时 IP 不变 NAT（端口映射）：Kubernetes 原生 NAT 网络方案 Host（主机网络）：Kubernetes 原生 Host 网络方案 Service：勾选【启用】按钮，配置负载端口访问 注意：如果不勾选【启用】则不会创建Service 服务访问方式：选择是【仅在集群内部访问】该负载还是集群外部通过【主机端口访问】该负载 仅在集群内访问：使用 Service 的 ClusterIP 模式，自动分配 Service 网段中的 IP，用于集群内访问。数据库类等服务如 MySQL 可以选择集群内访问，以保证服务网络隔离 主机端口访问：提供一个主机端口映射到容器的访问方式，支持 TCP、UDP、Ingress。可用于业务定制上层 LB 转发到 Node Headless Service：不创建用于集群内访问的ClusterIP，访问Service名称时返回后端Pods IP地址，用于适配自有的服务发现机制。解析域名时返回相应 Pod IP 而不是 Cluster IP 端口映射：输入负载要暴露的端口并指定通信协议类型（容器和服务端口建议都使用80） Session Affinity: 点击【显示高级设置】出现，会话保持，设置会话保持后，会根据请求IP把请求转发给这个IP之前访问过的Pod。默认None，按需使用 单击【创建Workload】，完成创建。如下图所示： 当“运行/期望Pod数量”相等时，即表示 Deployment 下的所有 Pod 已创建完成。 更新 Deployment 更新 YAML 登录TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择需要更新的【业务】下相应的【命名空间】，展开【工作负载】列表，进入【Deployment】管理页面。如下图所示： 在需要更新 YAML 的 Deployment 行中，单击【更多】>【编辑YAML】，进入“更新 Deployment” 页面。如下图所示： 在 “更新Deployment” 页面，编辑 YAML，单击【完成】，即可更新 YAML。如下图所示： 回滚 Deployment 登录 TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择要变更的业务下相应的【命名空间】，展开【工作负载】列表，进入【 Deployment】 管理页面，点击进入要回滚的 Deployment 详情页面，单击【修订历史】。如下图所示： 选择合适版本进行回顾。 在弹出的 “回滚资源” 提示框中，单击【确定】即可完成回滚。 调整 Pod 数量 登录 TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择要变更的业务下相应的命名空间，展开工作负载列表，进入 Deployment 管理页面。 点击 Deployment 列表操作栏的【更新实例数量】按钮。如下图所示： 根据实际需求调整 Pod 数量，单击【更新实例数目】即可完成调整。 查看Deployment监控数据 登录 TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】，。 选择要变更的业务下相应的【命名空间】，点击进入 【Deployment】 管理页面。 单击【监控】按钮，在弹出的工作负载监控页面选择工作负载查看监控信息。如下图所示： Kubectl 操作 Deployment 指引 YAML 示例 apiVersion: apps/v1beta2 kind: Deployment metadata: name: nginx-deployment namespace: default labels: app: nginx-deployment spec: replicas: 3 selector: matchLabels: app: nginx-deployment template: metadata: labels: app: nginx-deployment spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 kind：标识 Deployment 资源类型。 metadata：Deployment 的名称、Namespace、Label 等基本信息。 metadata.annotations：对 Deployment 的额外说明，可通过该参数设置腾讯云 TKE 的额外增强能力。 spec.replicas：Deployment 管理的 Pod 数量。 spec.selector：Deployment 管理 Seletor 选中的 Pod 的 Label。 spec.template：Deployment 管理的 Pod 的详细模板配置。 更多参数详情可查看 Kubernetes Deployment 官方文档。 Kubectl 创建 Deployment 参考 YAML 示例，准备 Deployment YAML 文件。 安装 Kubectl，并连接集群。操作详情请参考 通过 Kubectl 连接集群。 执行以下命令，创建 Deployment YAML 文件。kubectl create -f 【Deployment YAML 文件名称】 例如，创建一个文件名为 nginx.Yaml 的 Deployment YAML 文件，则执行以下命令：kubectl create -f nginx.yaml 执行以下命令，验证创建是否成功。kubectl get deployments 返回类似以下信息，即表示创建成功。NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE first-workload 1 1 1 0 6h ng 1 1 1 1 42m Kubectl 更新 Deployment 通过 Kubectl 更新 Deployment 有以下三种方法。其中，方法一 和 方法二 均支持 Recreate 和 RollingUpdate 两种更新策略。 Recreate 更新策略为先销毁全部 Pod，再重新创建 Deployment。 RollingUpdate 更新策略为滚动更新策略，逐个更新 Deployment 的 Pod。RollingUpdate 还支持暂停、设置更新时间间隔等。 方法一 执行以下命令，更新 Deployment。 kubectl edit deployment/【name】 此方法适用于简单的调试验证，不建议在生产环境中直接使用。您可以通过此方法更新任意的 Deployment 参数。 方法二 执行以下命令，更新指定容器的镜像。 kubectl set image deployment/【name】 【containerName】=【image:tag】 建议保持 Deployment 的其他参数不变，业务更新时，仅更新容器镜像。 方法三 执行以下命令，滚动更新指定资源。 kubectl rolling-update 【NAME】 -f 【FILE】 更多滚动更新可参见 滚动更新说明。 Kubectl 回滚 Deployment 执行以下命令，查看 Deployment 的更新历史。kubectl rollout history deployment/【name】 执行以下命令，查看指定版本详情。kubectl rollout history deployment/【name】 --revision=【REVISION】 执行以下命令，回滚到前一个版本。kubectl rollout undo deployment/【name】 如需指定回滚版本号，可执行以下命令。kubectl rollout undo deployment/【name】 --to-revision=【REVISION】 Kubectl 调整 Pod 数量 手动更新 Pod 数量 执行以下命令，手动更新 Pod 数量。 kubectl scale deployment 【NAME】 --replicas=【NUMBER】 自动更新 Pod 数量 前提条件 开启集群中的 HPA 功能。TKE 创建的集群默认开启 HPA 功能。 操作步骤 执行以下命令，设置 Deployment 的自动扩缩容。 kubectl autoscale deployment 【NAME】 --min=10 --max=15 --cpu-percent=80 Kubectl 删除 Deployment 执行以下命令，删除 Deployment。 kubectl delete deployment 【NAME】 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:04:52 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-2-工作负载/3-2-1-2-2-StatefulSet.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-2-工作负载/3-2-1-2-2-StatefulSet.html","title":"StatefulSet","keywords":"","body":"简介 StatefulSet 主要用于管理有状态的应用，创建的 Pod 拥有根据规范创建的持久型标识符。Pod 迁移或销毁重启后，标识符仍会保留。 在需要持久化存储时，您可以通过标识符对存储卷进行一一对应。如果应用程序不需要持久的标识符，建议您使用 Deployment 部署应用程序。 StatefulSet 控制台操作指引 创建 StatefulSet 登录TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择需要创建StatefulSet的【业务】下相应的【命名空间】，展开【工作负载】下拉项，进入【StatefulSet】管理页面。如下图所示： 单击【新建】，进入 “新建Workload” 页面。根据实际需求，设置 StatefulSet 参数。 关键参数信息如下，其中必填项为工作负载名、实例内容器的名称和镜像： 工作负载名：输入自定义名称。 标签：给工作负载添加标签 命名空间：根据实际需求进行选择。 类型：选择【StatefulSet】。 数据卷：根据需求，为负载添加数据卷为容器提供存，目前支持临时路径、主机路径、云硬盘数据卷、文件存储NFS、配置文件、PVC，还需挂载到容器的指定路径中 临时目录：主机上的一个临时目录，生命周期和Pod一致 主机路径：主机上的真实路径，可以重复使用，不会随Pod一起销毁 NFS盘：挂载外部NFS到Pod，用户需要指定相应NFS地址，格式：127.0.0.1:/data ConfigMap：用户在业务Namespace下创建的ConfigMap Secret：用户在业务namespace下创建的Secret PVC：用户在业务namespace下创建的PVC 实例内容器：根据实际需求，为 StatefulSet的一个 Pod 设置一个或多个不同的容器。 名称：自定义 镜像：根据实际需求进行选择 镜像版本（Tag）：根据实际需求进行填写，不填默认为latest CPU/内存限制：可根据 Kubernetes 资源限制 进行设置 CPU 和内存的限制范围，提高业务的健壮性（建议使用默认值） GPU限制：如容器内需要使用GPU，此处填GPU需求 环境变量：用于设置容器内的变量，变量名只能包含大小写字母、数字及下划线，并且不能以数字开头 新增变量：自己设定变量键值对 引用ConfigMap/Secret：引用已有键值对 高级设置：可设置 “工作目录”、“运行命令”、“运行参数”、“镜像更新策略”、“容器健康检查”和“特权级”等参数。这里介绍一下镜像更新策略。 镜像更新策略：提供以下3种策略，请按需选择 若不设置镜像拉取策略，当镜像版本为空或 latest 时，使用 Always 策略，否则使用 IfNotPresent 策略 Always：总是从远程拉取该镜像 IfNotPresent：默认使用本地镜像，若本地无该镜像则远程拉取该镜像 Never：只使用本地镜像，若本地没有该镜像将报异常 实例数量：根据实际需求选择调节方式，设置实例数量。 手动调节：直接设定实例个数 自动调节：根据设定的触发条件自动调节实例个数，目前支持根据CPU、内存利用率和利用量出入带宽等调节实例个数 定时调节：根据Crontab 语法周期性设置实例个数 imagePullSecrets：镜像拉取密钥，用于拉取用户的私有镜像 节点调度策略：根据配置的调度规则，将Pod调度到预期的节点。支持指定节点调度和条件选择调度 注释（Annotations）：给StatefulSet添加相应Annotation，如用户信息等 网络模式：选择Pod网络模式 OverLay（虚拟网络）：基于 IPIP 和 Host Gateway 的 Overlay 网络方案 FloatingIP（浮动 IP）：支持容器、物理机和虚拟机在同一个扁平面中直接通过IP进行通信的 Underlay 网络方案。提供了 IP 漂移能力，支持 Pod 重启或迁移时 IP 不变 NAT（端口映射）：Kubernetes 原生 NAT 网络方案 Host（主机网络）：Kubernetes 原生 Host 网络方案 Service：勾选【启用】按钮，配置负载端口访问 注意：如果不勾选【启用】则不会创建Service 服务访问方式：选择是【仅在集群内部访问】该负载还是集群外部通过【主机端口访问】该负载 仅在集群内访问：使用 Service 的 ClusterIP 模式，自动分配 Service 网段中的 IP，用于集群内访问。数据库类等服务如 MySQL 可以选择集群内访问，以保证服务网络隔离 主机端口访问：提供一个主机端口映射到容器的访问方式，支持 TCP、UDP、Ingress。可用于业务定制上层 LB 转发到 Node Headless Service：不创建用于集群内访问的ClusterIP，访问Service名称时返回后端Pods IP地址，用于适配自有的服务发现机制。解析域名时返回相应 Pod IP 而不是 Cluster IP 端口映射：输入负载要暴露的端口并指定通信协议类型（容器和服务端口建议都使用80） Session Affinity: 点击【显示高级设置】出现，会话保持，设置会话保持后，会根据请求IP把请求转发给这个IP之前访问过的Pod。默认None，按需使用 单击【创建Workload】，完成创建。 更新 StatefulSet 更新 YAML 登录TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择需要更新的【业务】下相应的【命名空间】，展开【工作负载】列表，进入【StatefulSet】管理页面。 在需要更新 YAML 的 StatefulSet 行中，选择【更多】>【编辑YAML】，进入更新 StatefulSet 页面。 在 “更新StatefulSet” 页面编辑 YAML，并单击【完成】即可更新 YAML。 Kubectl 操作 StatefulSet 指引 YAML 示例 apiVersion: v1 kind: Service ## 创建一个 Headless Service，用于控制网络域 metadata: name: nginx namespace: default labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet ### 创建一个 Nginx的StatefulSet metadata: name: web namespace: default spec: selector: matchLabels: app: nginx serviceName: \"nginx\" replicas: 3 # by default is 1 template: metadata: labels: app: nginx spec: terminationGracePeriodSeconds: 10 containers: - name: nginx image: nginx:latest ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: \"cbs\" resources: requests: storage: 10Gi kind：标识 StatefulSet 资源类型。 metadata：StatefulSet 的名称、Label等基本信息。 metadata.annotations：对 StatefulSet 的额外说明，可通过该参数设置腾讯云 TKE 的额外增强能力。 spec.template：StatefulSet 管理的 Pod 的详细模板配置。 spec.volumeClaimTemplates：提供创建 PVC&PV 的模板。 更多参数详情可查看 Kubernetes StatefulSet 官方文档。 创建 StatefulSet 参考 YAML 示例，准备 StatefulSet YAML 文件。 安装 Kubectl，并连接集群。操作详情请参考 通过 Kubectl 连接集群。 执行以下命令，创建 StatefulSet YAML 文件。kubectl create -f StatefulSet YAML 文件名称 例如，创建一个文件名为 web.yaml 的 StatefulSet YAML 文件，则执行以下命令：kubectl create -f web.yaml 执行以下命令，验证创建是否成功。kubectl get StatefulSet 返回类似以下信息，即表示创建成功。NAME DESIRED CURRENT AGE test 1 1 10s 更新 StatefulSet 执行以下命令，查看 StatefulSet 的更新策略类型。 kubectl get ds/ -o go-template='{{.spec.updateStrategy.type}}{{\"\\n\"}}' StatefulSet 有以下两种更新策略类型： OnDelete：默认更新策略。该更新策略在更新 StatefulSet 后，需手动删除旧的 StatefulSet Pod 才会创建新的 StatefulSet Pod。 RollingUpdate：支持 Kubernetes 1.7或更高版本。该更新策略在更新 StatefulSet 模板后，旧的 StatefulSet Pod 将被终止，并且以滚动更新方式创建新的 StatefulSet Pod（Kubernetes 1.7或更高版本）。 方法一 执行以下命令，更新 StatefulSet。 kubectl edit StatefulSet/[name] 此方法适用于简单的调试验证，不建议在生产环境中直接使用。您可以通过此方法更新任意的 StatefulSet 参数。 方法二 执行以下命令，更新指定容器的镜像。 kubectl patch statefulset --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/image\", \"value\":\"\"}]' 建议保持 StatefulSet 的其他参数不变，业务更新时，仅更新容器镜像。 如果更新的 StatefulSet 是滚动更新方式的策略，可执行以下命令查看更新状态： kubectl rollout status sts/ 删除 StatefulSet 执行以下命令，删除 StatefulSet。 kubectl delete StatefulSet [NAME] --cascade=false --cascade=false 参数表示 Kubernetes 仅删除 StatefulSet，且不删除任何 Pod。如需删除 Pod，则执行以下命令： kubectl delete StatefulSet [NAME] 更多 StatefulSet 相关操作可查看 Kubernetes官方指引。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:05:12 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-2-工作负载/3-2-1-2-3-DaomonSet.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-2-工作负载/3-2-1-2-3-DaomonSet.html","title":"DaomonSet","keywords":"","body":"简介 DaemonSet 主要用于部署常驻集群内的后台程序，例如节点的日志采集。DaemonSet 保证在所有或部分节点上均运行指定的 Pod。 新节点添加到集群内时，也会有自动部署 Pod；节点被移除集群后，Pod 将自动回收。 调度说明 若配置了 Pod 的 nodeSelector 或 affinity 参数，DaemonSet 管理的 Pod 将按照指定的调度规则调度。若未配置 Pod 的 nodeSelector 或 affinity 参数，则将在所有的节点上部署 Pod。 DaemonSet 控制台操作指引 创建 DaemonSet 登录TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择需要创建DaemonSet的【业务】下相应的【命名空间】，展开【工作负载】下拉项，进入【DaemonSet】管理页面。如下图所示： 单击【新建】，进入 “新建Workload” 页面。 根据实际需求，设置 DaemonSet 参数。关键参数信息如下： 工作负载名：输入自定义名称。 标签：给工作负载添加标签 命名空间：根据实际需求进行选择。 类型：选择【DaemonSet】。 数据卷：根据需求，为负载添加数据卷为容器提供存，目前支持临时路径、主机路径、云硬盘数据卷、文件存储NFS、配置文件、PVC，还需挂载到容器的指定路径中 临时目录：主机上的一个临时目录，生命周期和Pod一致 主机路径：主机上的真实路径，可以重复使用，不会随Pod一起销毁 NFS盘：挂载外部NFS到Pod，用户需要指定相应NFS地址，格式：127.0.0.1:/data ConfigMap：用户在业务Namespace下创建的ConfigMap Secret：用户在业务namespace下创建的Secret PVC：用户在业务namespace下创建的PVC 实例内容器：根据实际需求，为 DaemonSet 的一个 Pod 设置一个或多个不同的容器。 名称：自定义。 镜像：根据实际需求进行选择。 镜像版本（Tag）：根据实际需求进行填写。 CPU/内存限制：可根据 Kubernetes 资源限制 进行设置 CPU 和内存的限制范围，提高业务的健壮性。 GPU限制：如容器内需要使用GPU，此处填GPU需求 环境变量：用于设置容器内的变量，变量名只能包含大小写字母、数字及下划线，并且不能以数字开头 新增变量：自己设定变量键值对 引用ConfigMap/Secret：引用已有键值对 高级设置：可设置 “工作目录”、“运行命令”、“运行参数”、“镜像更新策略”、“容器健康检查”和“特权级”等参数。这里介绍一下镜像更新策略。 镜像更新策略：提供以下3种策略，请按需选择 若不设置镜像拉取策略，当镜像版本为空或 latest 时，使用 Always 策略，否则使用 IfNotPresent 策略 Always：总是从远程拉取该镜像 IfNotPresent：默认使用本地镜像，若本地无该镜像则远程拉取该镜像 Never：只使用本地镜像，若本地没有该镜像将报异常 imagePullSecrets：镜像拉取密钥，用于拉取用户的私有镜像 节点调度策略：根据配置的调度规则，将Pod调度到预期的节点。支持指定节点调度和条件选择调度 注释（Annotations）：给DaemonSet添加相应Annotation，如用户信息等 网络模式：选择Pod网络模式 OverLay（虚拟网络）：基于 IPIP 和 Host Gateway 的 Overlay 网络方案 FloatingIP（浮动 IP）：支持容器、物理机和虚拟机在同一个扁平面中直接通过IP进行通信的 Underlay 网络方案。提供了 IP 漂移能力，支持 Pod 重启或迁移时 IP 不变 NAT（端口映射）：Kubernetes 原生 NAT 网络方案 Host（主机网络）：Kubernetes 原生 Host 网络方案 单击【创建Workload】，完成创建。 更新 DaemonSet 更新 YAML 登录TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择需要更新的【业务】下相应的命名空间，展开【工作负载】列表，进入【DaemonSet】管理页面。 在需要更新 YAML 的 DaemonSet 行中，选择【更多】>【编辑YAML】，进入更新 DaemonSet 页面。 在 “更新DaemonSet” 页面编辑 YAML，并单击【完成】即可更新 YAML。 Kubectl 操作 DaemonSet 指引 YAML 示例 apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: k8s.gcr.io/fluentd-elasticsearch:1.20 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers !以上 YAML 示例引用于 https://kubernetes.io/docs/concepts/workloads/controllers/daemonset， 创建时可能存在容器镜像拉取不成功的情况，仅用于本文介绍 DaemonSet 的组成。 kind：标识 DaemonSet 资源类型。 metadata：DaemonSet 的名称、Label等基本信息。 metadata.annotations：DaemonSet 的额外说明，可通过该参数设置腾讯云 TKE 的额外增强能力。 spec.template：DaemonSet 管理的 Pod 的详细模板配置。 更多可查看 Kubernetes DaemonSet 官方文档。 Kubectl 创建 DaemonSet 参考 YAML 示例，准备 DaemonSet YAML 文件。 安装 Kubectl，并连接集群。操作详情请参考 通过 Kubectl 连接集群。 执行以下命令，创建 DaemonSet YAML 文件。kubectl create -f DaemonSet YAML 文件名称 例如，创建一个文件名为 fluentd-elasticsearch.yaml 的 DaemonSet YAML 文件，则执行以下命令：kubectl create -f fluentd-elasticsearch.yaml 执行以下命令，验证创建是否成功。kubectl get DaemonSet 返回类似以下信息，即表示创建成功。NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE frontend 0 0 0 0 0 app=frontend-node 16d Kubectl 更新 DaemonSet 执行以下命令，查看 DaemonSet 的更新策略类型。 kubectl get ds/ -o go-template='{{.spec.updateStrategy.type}}{{\"\\n\"}}' DaemonSet 有以下两种更新策略类型： OnDelete：默认更新策略。该更新策略在更新 DaemonSet 后，需手动删除旧的 DaemonSet Pod 才会创建新的DaemonSet Pod。 RollingUpdate：支持 Kubernetes 1.6或更高版本。该更新策略在更新 DaemonSet 模板后，旧的 DaemonSet Pod 将被终止，并且以滚动更新方式创建新的 DaemonSet Pod。 方法一 执行以下命令，更新 DaemonSet。 kubectl edit DaemonSet/[name] 此方法适用于简单的调试验证，不建议在生产环境中直接使用。您可以通过此方法更新任意的 DaemonSet 参数。 方法二 执行以下命令，更新指定容器的镜像。 kubectl set image ds/[daemonset-name][container-name]=[container-new-image] 建议保持 DaemonSet 的其他参数不变，业务更新时，仅更新容器镜像。 Kubectl 回滚 DaemonSet 执行以下命令，查看 DaemonSet 的更新历史。kubectl rollout history daemonset /[name] 执行以下命令，查看指定版本详情。kubectl rollout history daemonset /[name] --revision=[REVISION] 执行以下命令，回滚到前一个版本。kubectl rollout undo daemonset /[name] 如需指定回滚版本号，可执行以下命令。kubectl rollout undo daemonset /[name] --to-revision=[REVISION] Kubectl 删除 DaemonSet 执行以下命令，删除 DaemonSet。 kubectl delete DaemonSet [NAME] TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:05:31 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-2-工作负载/3-2-1-2-4-Job.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-2-工作负载/3-2-1-2-4-Job.html","title":"Job","keywords":"","body":"简介 Job 控制器会创建 1-N 个 Pod，这些 Pod 按照运行规则运行，直至运行结束。Job 可用于批量计算、数据分析等场景。通过设置重复执行次数、并行度、重启策略等满足业务述求。 Job 执行完成后，不再创建新的 Pod，也不会删除 Pod，您可在 “日志” 中查看已完成的 Pod 的日志。如果您删除了 Job，Job 创建的 Pod 也会同时被删除，将查看不到该 Job 创建的 Pod 的日志。 Job 控制台操作指引 创建 Job 登录TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择需要创建 Job 的【业务】下相应的【命名空间】，展开【工作负载】下拉项，进入【 Job】 管理页面。 单击【新建】，进入 “新建Workload” 页面。如下图所示： 根据实际需求，设置 Job 参数。关键参数信息如下： 工作负载名：输入自定义名称。 标签：给工作负载添加标签 命名空间：根据实际需求进行选择。 类型：选择【Job（单次任务）】。 Job设置 重复执行次数：Job 管理的 Pod 需要重复执行的次数。 并行度：Job 并行执行的 Pod 数量。 失败重启策略：Pod下容器异常推出后的重启策略。 Never：不重启容器，直至 Pod 下所有容器退出。 OnFailure：Pod 继续运行，容器将重新启动。 数据卷：根据需求，为负载添加数据卷为容器提供存，目前支持临时路径、主机路径、云硬盘数据卷、文件存储NFS、配置文件、PVC，还需挂载到容器的指定路径中 临时目录：主机上的一个临时目录，生命周期和Pod一致 主机路径：主机上的真实路径，可以重复使用，不会随Pod一起销毁 NFS盘：挂载外部 NFS 到 Pod，用户需要指定相应 NFS 地址，格式：127.0.0.1:/data ConfigMap：用户在业务Namespace下创建的ConfigMap Secret：用户在业务namespace下创建的Secret PVC：用户在业务namespace下创建的PVC 实例内容器：根据实际需求，为 Job 的一个 Pod 设置一个或多个不同的容器。 名称：自定义。 镜像：根据实际需求进行选择。 镜像版本（Tag）：根据实际需求进行填写。 CPU/内存限制：可根据 Kubernetes 资源限制 进行设置 CPU 和内存的限制范围，提高业务的健壮性。 GPU限制：如容器内需要使用GPU，此处填GPU需求 环境变量：用于设置容器内的变量，变量名只能包含大小写字母、数字及下划线，并且不能以数字开头 新增变量：自己设定变量键值对 引用ConfigMap/Secret：引用已有键值对 高级设置：可设置 “工作目录”、“运行命令”、“运行参数”、“镜像更新策略”、“容器健康检查”和“特权级”等参数。这里介绍一下镜像更新策略。 镜像更新策略：提供以下3种策略，请按需选择 若不设置镜像拉取策略，当镜像版本为空或 latest 时，使用 Always 策略，否则使用 IfNotPresent 策略 Always：总是从远程拉取该镜像 IfNotPresent：默认使用本地镜像，若本地无该镜像则远程拉取该镜像 Never：只使用本地镜像，若本地没有该镜像将报异常 imagePullSecrets：镜像拉取密钥，用于拉取用户的私有镜像 节点调度策略：根据配置的调度规则，将Pod调度到预期的节点。支持指定节点调度和条件选择调度 注释（Annotations）：给Pod添加相应Annotation，如用户信息等 网络模式：选择Pod网络模式 OverLay（虚拟网络）：基于 IPIP 和 Host Gateway 的 Overlay 网络方案 FloatingIP（浮动 IP）：支持容器、物理机和虚拟机在同一个扁平面中直接通过IP进行通信的 Underlay 网络方案。提供了 IP 漂移能力，支持 Pod 重启或迁移时 IP 不变 NAT（端口映射）：Kubernetes 原生 NAT 网络方案 Host（主机网络）：Kubernetes 原生 Host 网络方案 单击【创建Workload】，完成创建。查看 Job 状态 登录 TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择需要创建 Job 的业务下相应的【命名空间】，展开【工作负载】下拉项，进入【 Job 】管理页面。 单击需要查看状态的【 Job 名称】，即可查看 Job 详情。 删除 Job Job 执行完成后，不再创建新的 Pod，也不会删除 Pod，您可在【业务管理】控制台下的【应用管理】下的 【日志】 中查看已完成的 Pod 的日志。如果您删除了 Job，Job 创建的 Pod 也会同时被删除，将查看不到该 Job 创建的 Pod 的日志。 Kubectl 操作 Job 指引 YAML 示例 apiVersion: batch/v1 kind: Job metadata: name: pi spec: completions: 2 parallelism: 2 template: spec: containers: - name: pi image: perl command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"] restartPolicy: Never backoffLimit: 4 kind：标识 Job 资源类型。 metadata：Job 的名称、Label等基本信息。 metadata.annotations：Job 的额外说明，可通过该参数设置腾讯云 TKE 的额外增强能力。 spec.completions：Job 管理的 Pod 重复执行次数。 spec.parallelism：Job 并行执行的 Pod 数。 spec.template：Job 管理的 Pod 的详细模板配置。 创建 Job 参考 YAML 示例，准备 Job YAML 文件。 安装 Kubectl，并连接集群。操作详情请参考 通过 Kubectl 连接集群。 创建 Job YAML 文件。kubectl create -f Job YAML 文件名称 例如，创建一个文件名为 pi.yaml 的 Job YAML 文件，则执行以下命令：kubectl create -f pi.yaml 执行以下命令，验证创建是否成功。kubectl get job 返回类似以下信息，即表示创建成功。NAME DESIRED SUCCESSFUL AGE job 1 0 1m 删除 Job 执行以下命令，删除 Job。 kubectl delete job [NAME] TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:05:41 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-2-工作负载/3-2-1-2-5-CronJob.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-2-工作负载/3-2-1-2-5-CronJob.html","title":"CronJob","keywords":"","body":"简介 一个 CronJob 对象类似于 crontab（cron table）文件中的一行。它根据指定的预定计划周期性地运行一个 Job，格式可以参考 Cron。 Cron 格式说明如下： # 文件格式说明 # ——分钟（0 - 59） # | ——小时（0 - 23） # | | ——日（1 - 31） # | | | ——月（1 - 12） # | | | | ——星期（0 - 6） # | | | | | # * * * * * CronJob 控制台操作指引 创建 CronJob 登录TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择需要创建CronJob的业务下相应的【命名空间】，展开【工作负载】下拉项，进入【CronJob】管理页面。如下图所示： 单击【创建】按钮，进入 新建Workload页面。 根据实际需求，设置 CronJob 参数。关键参数信息如下： 工作负载名：输入自定义名称。 标签：给工作负载添加标签 命名空间：根据实际需求进行选择。 类型：选择【CronJob（按照Cron的计划定时运行）】。 执行策略：根据 Cron 格式设置任务的定期执行策略。 Job设置 重复执行次数：Job 管理的 Pod 需要重复执行的次数。 并行度：Job 并行执行的 Pod 数量。 失败重启策略：Pod下容器异常推出后的重启策略。 Never：不重启容器，直至 Pod 下所有容器退出。 OnFailure：Pod 继续运行，容器将重新启动。 数据卷：根据需求，为负载添加数据卷为容器提供存，目前支持临时路径、主机路径、云硬盘数据卷、文件存储NFS、配置文件、PVC，还需挂载到容器的指定路径中 临时目录：主机上的一个临时目录，生命周期和Pod一致 主机路径：主机上的真实路径，可以重复使用，不会随Pod一起销毁 NFS盘：挂载外部NFS到Pod，用户需要指定相应NFS地址，格式：127.0.0.1:/data ConfigMap：用户在业务Namespace下创建的ConfigMap Secret：用户在业务namespace下创建的Secret PVC：用户在业务namespace下创建的PVC 实例内容器：根据实际需求，为 CronJob 的一个 Pod 设置一个或多个不同的容器。 名称：自定义。 镜像：根据实际需求进行选择。 镜像版本（Tag）：根据实际需求进行填写。 CPU/内存限制：可根据 Kubernetes 资源限制 进行设置 CPU 和内存的限制范围，提高业务的健壮性。 GPU限制：如容器内需要使用GPU，此处填GPU需求 环境变量：用于设置容器内的变量，变量名只能包含大小写字母、数字及下划线，并且不能以数字开头 新增变量：自己设定变量键值对 引用ConfigMap/Secret：引用已有键值对 高级设置：可设置 “工作目录”、“运行命令”、“运行参数”、“镜像更新策略”、“容器健康检查”和“特权级”等参数。这里介绍一下镜像更新策略。 镜像更新策略：提供以下3种策略，请按需选择 若不设置镜像拉取策略，当镜像版本为空或 latest 时，使用 Always 策略，否则使用 IfNotPresent 策略 Always：总是从远程拉取该镜像 IfNotPresent：默认使用本地镜像，若本地无该镜像则远程拉取该镜像 Never：只使用本地镜像，若本地没有该镜像将报异常 imagePullSecrets：镜像拉取密钥，用于拉取用户的私有镜像 节点调度策略：根据配置的调度规则，将Pod调度到预期的节点。支持指定节点调度和条件选择调度 注释（Annotations）：给Pod添加相应Annotation，如用户信息等 网络模式：选择Pod网络模式 OverLay（虚拟网络）：基于 IPIP 和 Host Gateway 的 Overlay 网络方案 FloatingIP（浮动 IP）：支持容器、物理机和虚拟机在同一个扁平面中直接通过IP进行通信的 Underlay 网络方案。提供了 IP 漂移能力，支持 Pod 重启或迁移时 IP 不变 NAT（端口映射）：Kubernetes 原生 NAT 网络方案 Host（主机网络）：Kubernetes 原生 Host 网络方案 单击【创建Workload】，完成创建。 查看 CronJob 状态 登录TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择需要创建CronJob的【业务】下相应的【命名空间】，展开【工作负载】下拉项，进入【CronJob】管理页面。 单击需要查看状态的 CronJob 名称，即可查看 CronJob 详情。 Kubectl 操作 CronJob 指引 YAML 示例 apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure kind：标识 CronJob 资源类型。 metadata：CronJob 的名称、Label等基本信息。 metadata.annotations：对 CronJob 的额外说明，可通过该参数设置腾讯云 TKE 的额外增强能力。 spec.schedule：CronJob 执行的 Cron 的策略。 spec.jobTemplate：Cron 执行的 Job 模板。 创建 CronJob 方法一 参考 YAML 示例，准备 CronJob YAML 文件。 安装 Kubectl，并连接集群。操作详情请参考 通过 Kubectl 连接集群。 执行以下命令，创建 CronJob YAML 文件。kubectl create -f CronJob YAML 文件名称 例如，创建一个文件名为 cronjob.yaml 的 CronJob YAML 文件，则执行以下命令：kubectl create -f cronjob.yaml 方法二 通过执行kubectl run命令，快速创建一个 CronJob。 例如，快速创建一个不需要写完整配置信息的 CronJob，则执行以下命令：kubectl run hello --schedule=\"*/1 * * * *\" --restart=OnFailure --image=busybox -- /bin/sh -c \"date; echo Hello\" 执行以下命令，验证创建是否成功。kubectl get cronjob [NAME] 返回类似以下信息，即表示创建成功。NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE cronjob * * * * * False 0 15s 删除 CronJob ! 执行此删除命令前，请确认是否存在正在创建的 Job，否则执行该命令将终止正在创建的 Job。 执行此删除命令时，已创建的 Job 和已完成的 Job 均不会被终止或删除。 如需删除 CronJob 创建的 Job，请手动删除。 执行以下命令，删除 CronJob。 kubectl delete cronjob [NAME] TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:05:57 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-2-工作负载/3-2-1-2-6-TApp.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-2-工作负载/3-2-1-2-6-TApp.html","title":"TApp","keywords":"","body":"简介 Kubernetes现有应用类型（如：Deployment、StatefulSet等）无法满足很多非微服务应用的需求，比如：操作（升级、停止等）应用中的指定pod、应用支持多版本的pod。如果要将这些应用改造为适合于这些workload的应用，需要花费很大精力，这将使大多数用户望而却步。 为解决上述复杂应用管理场景，TKEStack基于Kubernetes CRD开发了一种新的应用类型TAPP，它是一种通用类型的workload，同时支持service和batch类型作业，满足绝大部分应用场景，它能让用户更好的将应用迁移到Kubernetes集群。 查询TApp可查看更多相关信息 TApp控制台操作指引 创建 TApp 注意：使用前提，在【扩展组件】安装TApp 登录TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择需要创建TApp的【业务】下相应的【命名空间】，展开【工作负载】下拉项，进入【TApp】管理页面。如下图所示： 单击【新建】，进入 “新建Workload” 页面。根据实际需求，设置 TApp 参数。关键参数信息如下，其中必填项为工作负载名、实例内容器的名称和镜像： 工作负载名：输入自定义名称。 标签：给工作负载添加标签 命名空间：根据实际需求进行选择。 类型：选择【TApp】。 节点异常策略 迁移，调度策略与Deployment一致，Pod会迁移到新的节点 不迁移，调度策略与StatefulSel一致，异常pod不会被迁移 数据卷：根据需求，为负载添加数据卷为容器提供存，目前支持临时路径、主机路径、云硬盘数据卷、文件存储NFS、配置文件、PVC，还需挂载到容器的指定路径中 临时目录：主机上的一个临时目录，生命周期和Pod一致 主机路径：主机上的真实路径，可以重复使用，不会随Pod一起销毁 NFS盘：挂载外部NFS到Pod，用户需要指定相应NFS地址，格式：127.0.0.1:/data ConfigMap：用户在业务Namespace下创建的ConfigMap Secret：用户在业务namespace下创建的Secret PVC：用户在业务namespace下创建的PVC 实例内容器：根据实际需求，为 TApp 的一个 Pod 设置一个或多个不同的容器。 名称：自定义 镜像：根据实际需求进行选择 镜像版本（Tag）：根据实际需求进行填写，不填默认为latest CPU/内存限制：可根据 Kubernetes 资源限制 进行设置 CPU 和内存的限制范围，提高业务的健壮性（建议使用默认值） GPU限制：如容器内需要使用GPU，此处填GPU需求 环境变量：用于设置容器内的变量，变量名只能包含大小写字母、数字及下划线，并且不能以数字开头 新增变量：自己设定变量键值对 引用ConfigMap/Secret：引用已有键值对 高级设置：可设置 “工作目录”、“运行命令”、“运行参数”、“镜像更新策略”、“容器健康检查”和“特权级”等参数。这里介绍一下镜像更新策略。 镜像更新策略：提供以下3种策略，请按需选择 若不设置镜像拉取策略，当镜像版本为空或 latest 时，使用 Always 策略，否则使用 IfNotPresent 策略 Always：总是从远程拉取该镜像 IfNotPresent：默认使用本地镜像，若本地无该镜像则远程拉取该镜像 Never：只使用本地镜像，若本地没有该镜像将报异常 实例数量：根据实际需求选择调节方式，设置实例数量。 手动调节：直接设定实例个数 自动调节：根据设定的触发条件自动调节实例个数，目前支持根据CPU、内存利用率和利用量出入带宽等调节实例个数 定时调节：根据Crontab 语法周期性设置实例个数 imagePullSecrets：镜像拉取密钥，用于拉取用户的私有镜像 节点调度策略：根据配置的调度规则，将Pod调度到预期的节点。支持指定节点调度和条件选择调度 注释（Annotations）：给TApp添加相应Annotation，如用户信息等 网络模式：选择Pod网络模式 OverLay（虚拟网络）：基于 IPIP 和 Host Gateway 的 Overlay 网络方案 FloatingIP（浮动 IP）：支持容器、物理机和虚拟机在同一个扁平面中直接通过IP进行通信的 Underlay 网络方案。提供了 IP 漂移能力，支持 Pod 重启或迁移时 IP 不变 NAT（端口映射）：Kubernetes 原生 NAT 网络方案 Host（主机网络）：Kubernetes 原生 Host 网络方案 Service：勾选【启用】按钮，配置负载端口访问 注意：如果不勾选【启用】则不会创建Service 服务访问方式：选择是【仅在集群内部访问】该负载还是集群外部通过【主机端口访问】该负载 仅在集群内访问：使用 Service 的 ClusterIP 模式，自动分配 Service 网段中的 IP，用于集群内访问。数据库类等服务如 MySQL 可以选择集群内访问，以保证服务网络隔离 主机端口访问：提供一个主机端口映射到容器的访问方式，支持 TCP、UDP、Ingress。可用于业务定制上层 LB 转发到 Node Headless Service：不创建用于集群内访问的ClusterIP，访问Service名称时返回后端Pods IP地址，用于适配自有的服务发现机制。解析域名时返回相应 Pod IP 而不是 Cluster IP 端口映射：输入负载要暴露的端口并指定通信协议类型（容器和服务端口建议都使用80） Session Affinity: 点击【显示高级设置】出现，会话保持，设置会话保持后，会根据请求IP把请求转发给这个IP之前访问过的Pod。默认None，按需使用 单击【创建Workload】，完成创建。如下图所示： 当“运行/期望Pod数量”相等时，即表示 TApp 下的所有 Pod 已创建完成。 更新 TApp 更新 YAML 登录TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择需要更新的【业务】下相应的【命名空间】，展开【工作负载】列表，进入【TApp】管理页面。在需要更新 YAML 的 TApp 行中，单击【更多】>【编辑YAML】，进入“更新 TApp” 页面。如下图所示： 在 “更新TApp” 页面，编辑 YAML，单击【完成】，即可更新 YAML。如下图所示： 调整 Pod 数量 登录 TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择要变更的业务下相应的命名空间，展开工作负载列表，进入 TApp 管理页面。 点击 TApp 列表操作栏的【更新实例数量】按钮。如下图所示： 根据实际需求调整 Pod 数量，如3，单击页面下方的【更新实例数目】即可完成调整。 TApp特色功能-指定pod灰度升级 登录 TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择要变更的业务下相应的【命名空间】，展开【工作负载】列表，进入【 TApp】 管理页面，点击进入要灰度升级的 TApp名称。如下图所示： 如下图标签1所示，可选指定需要灰度升级的pod，然后点击下图中标签2 的【灰度升级】即可升级指定pod。 在弹出的 “回滚资源” 提示框中，单击【确定】即可完成回滚。 注意：此页面同时可完成指定Pod监控和删除 查询TApp可查看更多相关信息 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:06:11 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-2-工作负载/3-2-1-2-7-设置工作负载的资源限制.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-2-工作负载/3-2-1-2-7-设置工作负载的资源限制.html","title":"请求（Request）与 限制（Limit）","keywords":"","body":"请求（Request）与 限制（Limit） Request：容器使用的最小资源需求，作为容器调度时资源分配的判断依赖。只有当节点上可分配资源量 >= 容器资源请求数时才允许将容器调度到该节点。但 Request 参数不限制容器的最大可使用资源值。 Limit： 容器能使用的资源最大值。 ! 更多 Limit 和 Request 参数介绍，单击 查看详情。 CPU 限制说明 CPU 资源允许设置 CPU 请求和 CPU 限制的资源量，以核（U）为单位，允许为小数。 ! CPU Request 作为调度时的依据，在创建时为该容器在节点上分配 CPU 使用资源，称为 “已分配 CPU” 资源。 CPU Limit 限制容器 CPU 资源的上限，不设置表示不做限制（CPU Limit >= CPU Request）。 内存限制说明 内存资源只允许限制容器最大可使用内存量。以 MiB 为单位，允许为小数。 ! 内存 Request 作为调度时的依据，在创建时为该容器在节点上分配内存，称为 “已分配内存” 资源。 内存资源为不可伸缩资源。当节点上所有容器使用内存均超量时，存在 OOM 的风险。因此不设置 Limit 时，默认 Limit = Request，保证容器的正常运作。 CPU 使用量和 CPU 使用率 CPU 使用量为绝对值，表示实际使用的 CPU 的物理核数，CPU 资源请求和 CPU 资源限制的判断依据都是 CPU 使用量。 CPU 使用率为相对值，表示 CPU 的使用量与 CPU 单核的比值（或者与节点上总 CPU 核数的比值）。 使用示例 一个简单的示例说明 Request 和 Limit 的作用，测试集群包括1个 4U4G 的节点、已经部署的两个 Pod ( Pod1，Pod2 )，每个 Pod 的资源设置为（CPU Requst，CPU Limit，Memory Requst，Memory Limit）=（1U，2U，1G，1G）。（1.0G = 1000MiB） 节点上 CPU 和内存的资源使用情况如下图所示： 已经分配的 CPU 资源为：1U（分配 Pod1） + 1U（分配 Pod2） = 2U，剩余可以分配的 CPU 资源为2U。 已经分配的内存资源为：1G（分配 Pod1） + 1G（分配 Pod2） = 2G，剩余可以分配的内存资源为2G。 所以该节点可以再部署一个 ( CPU Requst， Memory Requst ) =( 2U，2G )的 Pod 部署，或者部署2个（CPU Requst，Memory Requst） = （1U，1G） 的 Pod 部署。 在资源限制方面，每个 Pod1 和 Pod2 使用资源的上限为 ( 2U，1G )，即在资源空闲的情况下，Pod 使用 CPU 的量最大能达到2U。 服务资源限制推荐 CCS 会根据您当前容器镜像的历史负载来推荐 Request 与 Limit 值，使用推荐值会保证您的容器更加平稳的运行，大大减小出现异常的概率。 推荐算法： 我们首先会取出过去7天当前容器镜像分钟级别负载，并辅以百分位统计第95%的值来最终确定推荐的 Request，Limit 为 Request 的2倍。 Request = Percentile(实际负载[7d],0.95) Limit = Request * 2 如果当前的样本数量（实际负载）不满足推荐计算的数量要求，我们会相应的扩大样本取值范围，尝试重新计算。例如，去掉镜像 tag，namespace，serviceName 等筛选条件。若经过多次计算后同样未能得到有效值，则推荐值为空。 推荐值为空： 在使用过程中，您会发现有部分值暂无推荐的情况，可能由于以下几点造成： 当前数据并不满足计算的需求，我们需要待计算的样本数量（实际负载）大于1440个，即有一天的数据。 推荐值小于您当前容器已经配置的 Request 或者 Limit。 ! 由于推荐值是根据历史负载来计算的，原则上，容器镜像运行真实业务的时间越长，推荐的值越准确。 使用推荐值创建服务，可能会因为集群资源不足造成容器无法调度成功。在保存时，须确认当前集群的剩余资源。 推荐值是建议值，您可以根据自己业务的实际情况做相应的调整。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:06:30 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-3-服务/":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-3-服务/","title":"服务","keywords":"","body":"服务 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:06:50 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-3-服务/3-2-1-3-1-Service.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-3-服务/3-2-1-3-1-Service.html","title":"Service","keywords":"","body":"简介 Service 定义访问后端 Pod 的访问方式，并提供固定的虚拟访问 IP。您可以在 Service 中通过设置来访问后端的 Pod，不同访问方式的服务可提供不同网络能力。 腾讯云容器服务（TKE）提供以下四种服务访问方式： 访问方式 说明 仅在集群内访问 使用 Service 的 ClusterIP 模式，自动分配 Service 网段中的 IP，用于集群内访问。数据库类等服务如 MySQL 可以选择集群内访问，以保证服务网络隔离。 创建完成后的服务可以通过服务名 + 服务端口访问服务。 主机端口访问 提供一个主机端口映射到容器的访问方式，支持 TCP、UDP、Ingress。可用于业务定制上层 LB 转发到 Node。 创建完成后的服务可以通过云服务器 IP+主机端口或服务名 + 服务端口访问服务。 集群内进行 Service 访问时，建议不要通过负载均衡 IP 进行访问，以避免出现访问不通的情况。 一般情况下，4层负载均衡（LB）会绑定多台 Node 作为 real server（rs） ，使用时需要限制 client 和 rs 不能存在于同一台云服务器上，否则会有一定概率导致报文回环出不去。 当 Pod 去访问 LB 时，Pod 就是源 IP，当其传输到内网时 LB 也不会做 snat 处理将源 IP 转化成 Node IP，那么 LB 收到报文也就不能判断是从哪个 Node 发送的，LB 的避免回环策略也就不会生效，所有的 rs 都可能被转发。当转发到 client 所在的 Node 上时，LB 就无法收到回包，从而导致访问不通。 集群内访问时，支持Headless Service，解析服务名时直接返回对应Pod IP而不是Cluster IP，可以适配自有的服务发现机制。 两种访问方式均支持Session Affinity，设置会话保持后，会根据请求IP把请求转发给这个IP之前访问过的Pod. Service 控制台操作指引 创建 Service 登录TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择需要创建 Service 的【业务】下相应的【命名空间】，展开【服务】列表，进入【Service】管理页面。 单击【新建】，进入 “新建 Service” 页面。如下图所示： 根据实际需求，设置 Service 参数。关键参数信息如下： 服务名称：自定义。 命名空间：根据实际需求进行选择。 访问设置：请参考 简介 并根据实际需求进行设置。 单击【创建服务】，完成创建。 更新 Service 更新 YAML 登录TKEStack，切换到业务管理控制台，选择左侧导航栏中的【应用管理】。 选择需要创建Service的业务下相应的命名空间，展开服务列表，进入Service管理页面。 在需要更新 YAML 的 Service 行中，单击【编辑YAML】，进入更新 Service 页面。 在 “更新Service” 页面，编辑 YAML，单击【完成】，即可更新 YAML。 Kubectl 操作 Service 指引 YAML 示例 kind: Service apiVersion: v1 metadata: name: my-service spec: selector: app: MyApp ports: - protocol: TCP port: 80 targetPort: 9376 type: LoadBalancer kind：标识 Service 资源类型。 metadata：Service 的名称、Label等基本信息。 spec.type：标识 Service 的被访问形式 ClusterIP：在集群内部公开服务，可用于集群内部访问。 NodePort：使用节点的端口映射到后端 Service，集群外可以通过节点 IP:NodePort 访问。 LoadBalancer：使用腾讯云提供的负载均衡器公开服务，默认创建公网 CLB， 指定 annotations 可创建内网 CLB。 ExternalName：将服务映射到 DNS，仅适用于 kube-dns1.7及更高版本。 创建 Service 参考 YAML 示例，准备 StatefulSet YAML 文件。 安装 Kubectl，并连接集群。操作详情请参考 通过 Kubectl 连接集群。 执行以下命令，创建 Service YAML 文件。kubectl create -f Service YAML 文件名称 例如，创建一个文件名为 my-service.yaml 的 Service YAML 文件，则执行以下命令：kubectl create -f my-service.yaml 执行以下命令，验证创建是否成功。kubectl get services 返回类似以下信息，即表示创建成功。NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 172.16.255.1 443/TCP 38d 更新 Service 方法一 执行以下命令，更新 Service。 kubectl edit service/[name] 方法二 手动删除旧的 Service。 执行以下命令，重新创建 Service。kubectl create/apply 删除 Service 执行以下命令，删除 Service。 kubectl delete service [NAME] .params{margin-bottom:0px !important;} TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:07:16 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-3-服务/3-2-1-3-2-Ingress.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-3-服务/3-2-1-3-2-Ingress.html","title":"Ingress","keywords":"","body":"简介 Ingress 是允许访问到集群内 Service 的规则的集合，您可以通过配置转发规则，实现不同 URL 可以访问到集群内不同的 Service。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:07:28 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-4-配置管理/":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-4-配置管理/","title":"配置管理","keywords":"","body":"配置管理 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:07:05 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-4-配置管理/3-2-1-4-1-ConfigMap.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-4-配置管理/3-2-1-4-1-ConfigMap.html","title":"ConfigMap","keywords":"","body":" 简介 通过 ConfigMap 您可以将配置和运行的镜像进行解耦，使得应用程序有更强的移植性。ConfigMap 是有 key-value 类型的键值对，您可以通过控制台的 Kubectl 工具创建对应的 ConfigMap 对象，可以通过挂载数据卷、环境变量或在容器的运行命令中使用 ConfigMap。 ConfigMap 有两种使用方式，创建负载时做为数据卷挂载到容器和作为环境变量映射到容器。 ConfigMap 控制台操作指引 创建 ConfigMap 登录TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择需要创建ConfigMap的【业务】下相应的【命名空间】，展开【配置管理】列表，进入ConfigMap管理页面。 单击【新建】，进入 “新建ConfigMap” 页面。如下图所示： 根据实际需求，设置 ConfigMap 参数。关键参数信息如下： 名称：自定义。 命名空间：根据实际需求进行选择命名空间类型 定义变量名和变量值。 单击【创建ConfigMap】，完成创建。 更新 ConfigMap 登录TKEStack，切换到业务管理控制台，选择左侧导航栏中的【应用管理】。 选择需要创建ConfigMap的业务下相应的命名空间，展开配置管理列表，进入ConfigMap管理页面。 在需要更新 YAML 的 ConfigMap 行中，单击【编辑YAML】，进入更新 ConfigMap 页面。 在 “更新ConfigMap” 页面，编辑 YAML，单击【完成】，即可更新 YAML。 如需修改 key-values，编辑 YAML 中 data 的参数值，单击【完成】，即可完成更新。 Kubectl 操作 ConfigMap 指引 YAML 示例 apiVersion: v1 data: key1: value1 key2: value2 key3: value3 kind: ConfigMap metadata: name: test-config namespace: default data：ConfigMap 的数据，以 key-value 形式呈现。 kind：标识 ConfigMap 资源类型。 metadata：ConfigMap 的名称、Label等基本信息。 metadata.annotations：ConfigMap 的额外说明，可通过该参数设置腾讯云 TKE 的额外增强能力。 创建 ConfigMap 方式一：通过 YAML 示例文件方式创建 参考 YAML 示例，准备 ConfigMap YAML 文件。 安装 Kubectl，并连接集群。操作详情请参考 通过 Kubectl 连接集群。 执行以下命令，创建 ConfigMap YAML 文件。kubectl create -f ConfigMap YAML 文件名称 例如，创建一个文件名为 web.yaml 的 ConfigMap YAML 文件，则执行以下命令：kubectl create -f web.yaml 执行以下命令，验证创建是否成功。kubectl get configmap 返回类似以下信息，即表示创建成功。NAME DATA AGE test 2 39d test-config 3 18d 方式二：通过执行命令方式创建 执行以下命令，在目录中创建 ConfigMap。 kubectl create configmap ：表示 ConfigMap 的名字。 ：表示目录、文件或者字面值。 更多参数详情可参见 Kubernetes configMap 官方文档。 使用 ConfigMap 方式一：数据卷使用 ConfigMap 类型 YAML 示例如下： apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx:latest volumeMounts: name: config-volume mountPath: /etc/config volumes: name: config-volume configMap: name: test-config ## 设置 ConfigMap 来源 ## items: ## 设置指定 ConfigMap 的 Key 挂载 ## key: key1 ## 选择指定 Key ## path: keys ## 挂载到指定的子路径 restartPolicy: Never 方式二：环境变量中使用 ConfigMap 类型 YAML 示例如下： apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx:latest env: - name: key1 valueFrom: configMapKeyRef: name: test-config ## 设置来源 ConfigMap 文件名 key: test-config.key1 ## 设置该环境变量的 Value 来源项 restartPolicy: Never TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:07:51 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-4-配置管理/3-2-1-4-2-Secret.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-4-配置管理/3-2-1-4-2-Secret.html","title":"Secret","keywords":"","body":"简介 Secret 可用于存储密码、令牌、密钥等敏感信息，降低直接对外暴露的风险。Secret 是 key-value 类型的键值对，您可以通过控制台的 Kubectl 工具创建对应的 Secret 对象，也可以通过挂载数据卷、环境变量或在容器的运行命令中使用 Secret。 Secret 控制台操作指引 创建 Secret 登录 TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】。 选择需要创建 Secret 的【业务】下相应的【命名空间】，展开【配置管理】列表，进入 Secret 管理页面。 单击【新建】，进入“新建 Secret ”页面。 在“新建 Secret ”页面，根据实际需求，进行如下参数设置。如下图所示： 名称：请输入自定义名称。 Secret类型：提供【Opaque】和【Dockercfg】两种类型，请根据实际需求进行选择。 Opaque：适用于保存秘钥证书和配置文件，Value 将以 base64 格式编码。 Dockercfg：适用于保存私有 Docker Registry 的认证信息。 生效范围：提供以下两种范围，请根据实际需求进行选择。 存量所有命名空间：不包括 kube-system、kube-public 和后续增量命名空间。 指定命名空间：支持选择当前集群下一个或多个可用命名空间。 内容：根据不同的 Secret 类型，进行配置。 当 Secret 类型为【Opaque】时：根据实际需求，设置变量名和变量值。 当 Secret 类型为【Dockercfg】时： 仓库域名：请根据实际需求输入域名或 IP。 用户名：请根据实际需求输入第三方仓库的用户名。 密码：请根据实际需求设置第三方仓库的登录密码。 如果本次为首次登录系统，则会新建用户，相关信息写入 ~/.dockercrg 文件中。 单击【创建 Secret】，即可完成创建。 使用 Secret Secret 在 Workload中有三种使用场景： 数据卷使用 Secret 类型 环境变量中使用 Secret 类型 使用第三方镜像仓库时引用 更新 Secret 登录 TKEStack，切换到业务管理控制台，选择左侧导航栏中的【应用管理】。 选择需要创建 Secret 的业务下相应的命名空间，展开配置管理列表，进入 Secret 管理页面。 在需要更新 YAML 的 Secret 行中，单击【编辑YAML】，进入更新 Secret 页面。 在“更新Secret”页面，编辑 YAML，并单击【完成】即可更新 YAML。 如需修改 key-values，则编辑 YAML 中 data 的参数值，并单击【完成】即可完成更新。 Kubectl 操作 Secret 指引 创建 Secret 方式一：通过指定文件创建 Secret 依次执行以下命令，获取 Pod 的用户名和密码。$ echo -n 'username' > ./username.txt $ echo -n 'password' > ./password.txt 执行 Kubectl 命令，创建 Secret。$ kubectl create secret generic test-secret --from-file=./username.txt --from-file=./password.txt secret \"testSecret\" created 执行以下命令，查看 Secret 详情。kubectl describe secrets/ test-secret 方式二：YAML 文件手动创建 ? 通过 YAML 手动创建 Secret，需提前将 Secret 的 data 进行 Base64 编码。 apiVersion: v1 kind: Secret metadata: name: test-secret type: Opaque data: username: dXNlcm5hbWU= ## 由echo -n 'username' | base64生成 password: cGFzc3dvcmQ= ## 由echo -n 'password' | base64生成 使用 Secret 方式一： 数据卷使用 Secret 类型 YAML 示例如下： apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx:latest volumeMounts: name: secret-volume mountPath: /etc/config volumes: name: secret-volume secret: name: test-secret ## 设置 secret 来源 ## items: ## 设置指定 secret的 Key 挂载 ## key: username ## 选择指定 Key ## path: group/user ## 挂载到指定的子路径 ## mode: 256 ## 设置文件权限 restartPolicy: Never 方式二： 环境变量中使用 Secret 类型 YAML 示例如下： apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx:latest env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: test-secret ## 设置来源 Secret 文件名 key: username ## 设置该环境变量的 Value 来源项 restartPolicy: Never 方法三：使用第三方镜像仓库时引用 YAML 示例如下： apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx:latest imagePullSecrets: - name: test-secret ## 设置来源 Secret 文件名 restartPolicy: Never TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:08:02 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-5-存储/":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-5-存储/","title":"存储","keywords":"","body":"存储 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:07:20 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-5-存储/3-2-1-5-1-PV和PVC.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-5-存储/3-2-1-5-1-PV和PVC.html","title":"PV和PVC","keywords":"","body":"简介 PersistentVolume（PV）：集群内的存储资源。例如，节点是集群的资源。PV 独立于 Pod 的生命周期，根据不同的 StorageClass 类型创建不同类型的 PV。 PersistentVolumeClaim（PVC）：集群内的存储请求。例如，PV 是 Pod 使用节点资源，PVC 则声明使用 PV 资源。当 PV 资源不足时，PVC 也可以动态创建 PV。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:08:54 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-5-存储/3-2-1-5-2-StorageClass.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-5-存储/3-2-1-5-2-StorageClass.html","title":"StorageClass","keywords":"","body":"简介 StorageClass 描述存储的类型，集群管理员可以为集群定义不同的存储类别。可通过 StorageClass 配合 PersistentVolumeClaim 可以动态创建需要的存储资源。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:09:09 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-6-事件.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-6-事件.html","title":"事件","keywords":"","body":"简介 日志针对的是容器。包括了 Kuberntes 集群的运行容器的日志情况和各类资源的调度情况，对维护人员日常观察资源的变更以及定位问题均有帮助。 日志控制台操作指引 登录 TKEStack，切换到【业务管理】控制台，点击【应用管理】，选择【日志】。 进入“日志”页面。 可以按照不用的命名空间和资源类型进行筛选。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:09:28 "},"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-7-日志.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-1-应用管理/3-2-1-7-日志.html","title":"日志","keywords":"","body":"简介 时间针对的是负载。Kubernetes Events 包括了 Kuberntes 集群的运行和各类资源的调度情况，对维护人员日常观察资源的变更以及定位问题均有帮助。 Event 控制台操作指引 登录 TKEStack，切换到【业务管理】控制台，点击【应用管理】中的【事件】，进入“事件”页面。 可以按照不用的命名空间和资源类型进行筛选。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:09:39 "},"zh/3-产品使用指南/3-2-业务侧/3-2-2-组织资源/":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-2-组织资源/","title":"组织资源","keywords":"","body":"组织资源 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:07:41 "},"zh/3-产品使用指南/3-2-业务侧/3-2-2-组织资源/3-2-2-1-仓库管理.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-2-组织资源/3-2-2-1-仓库管理.html","title":"仓库管理","keywords":"","body":"仓库管理 这里用户可以管理镜像仓库。 注意：TKEStack的【业务管理】控制台不支持命名空间的创建，可以在【平台管理】下的【组织资源】下的【镜像仓库管理】新建命名空间。 删除命名空间 登录 TKEStack。 切换至 【业务管理】控制台，选择 【组织资源】->【仓库管理】。点击列表最右侧【删除】按钮。如下图所示： 点击【确认】 镜像上传 登录 TKEStack。 切换至 【业务管理】控制台，选择 【组织资源】->【仓库管理】，查看“命名空间”列表。点击列表中命名空间【名称】。如下图所示： 此时进入了“镜像列表”页面，点击【镜像上传指引】按钮。如下图所示： 注意：此页面可以通过上传的镜像最右边的【删除】按钮来删除上传的镜像 根据指引内容，在物理节点上执行相应命令。如下图所示： TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:12:56 "},"zh/3-产品使用指南/3-2-业务侧/3-2-2-组织资源/3-2-2-2-访问凭证.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-2-组织资源/3-2-2-2-访问凭证.html","title":"访问凭证","keywords":"","body":"访问凭证 这里用户可以管理自己的凭据。 新建访问凭证 登录 TKEStack。 切换至 【业务管理】控制台，选择【组织资源】->【访问凭证】，点击【新建】按钮。 在弹出“创建访问凭证”页面，填写凭证信息。如下图所示： 凭证描述：描述当前凭证信息 过期时间：填写过期时间，选择小时/分钟为单位 单击【确认】按钮 停用/启用/删除访问凭证 登录 TKEStack。 切换至 【业务管理】控制台，选择 【组织资源】-> 【访问凭证】，查看“访问凭证”列表。单击列表右侧【禁用】/【启用】/【删除】按钮。如下图所示： 注意：点击【禁用】之后，【禁用】按钮就变成了【启用】 单击【确认】按钮 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:13:05 "},"zh/3-产品使用指南/3-2-业务侧/3-2-3-监控与告警/":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-3-监控与告警/","title":"监控与告警","keywords":"","body":"监控与告警 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:08:02 "},"zh/3-产品使用指南/3-2-业务侧/3-2-3-监控与告警/3-2-3-1-设置告警.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-3-监控与告警/3-2-3-1-设置告警.html","title":"告警设置","keywords":"","body":"告警设置 概念 这里用户配置平台告警。 前提条件 需要设置告警的命名空间所在的集群，应该先在【扩展组件】安装Prometheus组件 操作步骤 新建告警设置 登录 TKEStack。 切换至【业务管理】控制台，选择【监控&告警】-> 【告警设置】，查看“告警设置”列表。 选择相应【项目】和【namespace】，点击【新建】按钮。如下图所示： 在“新建策略”页面填写告警策略信息。如下图所示： 告警策略名称： 输入告警策略名称，最长60字符 策略类型： 选择告警策略应用类型 集群： 集群监控告警 Pod： Pod 监控告警 告警对象： 选择Pod相关的告警对象，支持对namespace下不同对deployment、stateful和daemonset 进行监控报警 按工作负载选择： 选择namespace下的某个工作负载 全部选择： 不区分namespace，全部监控 节点： 节点监控告警 统计周期： 选择数据采集周期，支持1、2、3、4、5分钟 指标： 选择告警指标，支持对监测值与指标值进行【大于/小于】比较，选择结果持续周期。如下图： 接收组： 选择接收组，当出现满足条件当报警信息时，向组内人员发送消息。接收组需要先在 通知设置 创建 通知方式： 选择通知渠道和消息模版。通知渠道 和 消息模版需要先在 通知设置 创建 添加通知方式 如需要添加多种通知方式，点击该按钮。 单击【提交】按钮。 复制告警设置 登录 TKEStack。 切换至【业务管理】控制台，选择 【监控&告警】->【告警设置】，查看告警设置列表。 选择相应【项目】和【namespace】，点击“告警设置”列表最右侧的【复制】按钮。如下图所示： 在复制策略页面，编辑告警策略信息。 单击【提交】按钮。编辑告警设置 登录 TKEStack。 切换至 【业务管理】控制台，选择 【监控&告警】->【告警设置】，查看告警设置列表。 选择相应【项目】和【namespace】，点击【告警名称】。如下图所示： 在“告警策略详情”页面，单击【基本信息】右侧的【编辑】按钮。如下图所示： 在更新策略页面，编辑策略信息。 单击【提交】按钮。删除告警设置 登录 TKEStack。 切换至 【业务管理】控制台，选择 【监控&告警】->【告警设置】，查看“告警设置”列表。 选择相应【项目】和【namespace】，点击列表最右侧的【删除】按钮。如下图所示： 在弹出的删除告警窗口，单击【确定】按钮。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:14:10 "},"zh/3-产品使用指南/3-2-业务侧/3-2-3-监控与告警/3-2-3-2-通知管理.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-3-监控与告警/3-2-3-2-通知管理.html","title":"通知设置","keywords":"","body":"通知设置 概念 这里用户配置平台通知。 操作步骤 这部分和【平台管理】控制台下【监控&告警】下的【通知设置】完全一致。故这里使用【平台管理】控制台下的截图。 通知设置 概念 这里用户配置平台通知。 操作步骤 通知渠道 新建通知渠道 登录 TKEStack。 切换至【业务管理】控制台，选择 【监控&告警】->【通知设置】->【通知渠道】，查看“通知渠道”列表。 点击【新建】按钮。如下图所示： 在“新建通知渠道”页面填写渠道信息。如下图所示： 名称： 填写渠道名称 渠道： 选择渠道类型，输入渠道信息 邮件： 邮件类型 email： 邮件发送放地址 password： 邮件发送方密码 smtpHost： smtp IP地址 smtpPort： smtp端口 tls： 是否tls加密 短信： 短信方式 appKey： 短信发送方的appKey sdkAppID： sdkAppID extend： extend 信息 微信公众号： 微信公众号方式 appID： 微信公众号appID appSecret： 微信公众号app密钥 单击【保存】按钮。 编辑通知渠道 登录 TKEStack。 切换至【业务管理】控制台，选择 【监控&告警】->【通知设置】->【通知渠道】，查看“通知渠道”列表。 单击渠道名称。如下图所示： 在“基本信息”页面，单击【基本信息】右侧的【编辑】按钮。如下图所示： 在“更新渠道通知”页面，编辑渠道信息。 单击【保存】按钮。 删除通知渠道 登录 TKEStack。 切换至【业务管理】控制台，选择 【监控&告警】->【通知设置】->【通知渠道】，查看“通知渠道”列表。 选择要删除的渠道，点击【删除】按钮。如下图所示： 单击删除窗口的【确定】按钮。 通知模板 新建通知模版 登录 TKEStack。 切换至【业务管理】控制台，选择 【监控&告警】->【通知设置】->【通知模板】，查看“通知模板”列表。 点击【新建】按钮。如下图所示： 在“新建通知模版”页面填写模版信息。如下图所示： 名称： 模版名称 渠道： 选择已创建的渠道 body： 填写消息body体 header： 填写消息标题 单击【保存】按钮。 编辑通知模版 登录 TKEStack。 切换至【业务管理】控制台，选择 【监控&告警】->【通知设置】->【通知模板】，查看“通知模板”列表。 单击模版名称。如下图所示： 在基本信息页面，单击【基本信息】右侧的【编辑】按钮。如下图所示： 在“更新通知模版”页面，编辑模版信息。 单击【保存】按钮。 删除通知模版 登录 TKEStack。 切换至【业务管理】控制台，选择 【监控&告警】->【通知设置】->【通知模板】，查看\"通知模板\"列表。 选择要删除的模版，点击【删除】按钮。如下图所示： 单击删除窗口的【确定】按钮。 接收人 新建接收人 登录 TKEStack。 切换至【业务管理】控制台，选择 【监控&告警】->【通知设置】->【接收人】，查看\"接收人\"列表。 点击【新建】按钮。如下图所示： 在“新建接收人”页面填写模版信息。如下图所示： 显示名称： 接收人显示名称 用户名： 接收人用户名 移动电话： 手机号 电子邮件： 接收人邮箱 微信OpenID： 接收人微信ID 单击【保存】按钮。 编辑接收人信息 登录 TKEStack。 切换至【业务管理】控制台，选择 【监控&告警】->【通知设置】->【接收人】，查看“接收人”列表。 单击接收人名称。如下图所示： 在“基本信息”页面，单击【基本信息】右侧的【编辑】按钮。如下图所示： 在“更新接收人”页面，编辑接收人信息。 单击【保存】按钮。 删除接收人 登录 TKEStack。 切换至【业务管理】控制台，选择 【监控&告警】->【通知设置】->【接收人】，查看“接收人”列表。 选择要删除的接收人，点击【删除】按钮。如下图所示： 单击删除窗口的【确定】按钮。 接收组 新建接收组 登录 TKEStack。 切换至【业务管理】控制台，选择 【监控&告警】->【通知设置】->【接收组】，查看“接收组”列表。 点击【新建】按钮。如下图所示： 在“新建接收组”页面填写模版信息。如下图所示： 名称： 接收组显示名称 接收组： 从列表里选择接收人。如没有想要的接收人，请在接收人里创建。 单击【保存】按钮。 编辑接收组信息 登录 TKEStack。 切换至【业务管理】控制台，选择 【监控&告警】->【通知设置】->【接收组】，查看“接收组”列表。 单击接收组名称。如下图所示： 在“基本信息”页面，单击【基本信息】右侧的【编辑】按钮。如下图所示： 在“更新接收组”页面，编辑接收组信息。 单击【保存】按钮。 删除接收组 登录 TKEStack。 切换至【业务管理】控制台，选择 【监控&告警】->【通知设置】->【接收组】，查看“接收组”列表。 选择要删除的接收组，点击【删除】按钮。如下图所示： 单击删除窗口的【确定】按钮。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:14:22 "},"zh/3-产品使用指南/3-2-业务侧/3-2-4-运维中心/":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-4-运维中心/","title":"运维中心","keywords":"","body":"运维中心 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:08:17 "},"zh/3-产品使用指南/3-2-业务侧/3-2-4-运维中心/3-2-4-1-日志采集.html":{"url":"zh/3-产品使用指南/3-2-业务侧/3-2-4-运维中心/3-2-4-1-日志采集.html","title":"运维中心","keywords":"","body":"运维中心 这里用户可以管理日志采集。 新建日志采集规则 1. 登录 TKEStack。 2. 切换至【业务管理】控制台，选择 【运维中心】->【日志采集】。 3. 选择相应【业务】和【命名空间】，单击【新建】按钮。如下图所示： ![新建日志采集按钮](https://github.com/tkestack/tke/blob/master/docs/images/logcollector.png?raw=true) 4. 在“新建日志采集”页面填写日志采集信息。如下图所示： ![新建日志采集](https://github.com/tkestack/tke/blob/master/docs/images/lognew.png?raw=true) + **收集规则名称：** 输入规则名，1～63字符，只能包含小写字母、数字及分隔符(\"-\")，且必须以小写字母开头，数字或小写字母结尾 + **业务：** 选择所属业务员 + **类型：** 选择采集类型 + **容器标准输出：** 容器Stdout信息采集 + **日志源：** 可以选择所有容器或者某个namespace下的所有容器/工作负载 + **所有容器：** 所有容器 + **指定容器：** 某个Namespace下的所有容器或者工作负载 + **容器文件路径：** 容器内文件内容采集 + **日志源：** 可以采集具体容器内的某个文件路径下的文件内容 + **工作负载选项：** 选择某个namespace下的某种工作负载类型下的某个工作负载 + **配置采集路径：** 选择某个容器下的某个文件路径 + **节点文件路径：** 收集节点上某个路径下的文件内容 + **日志源：** + **收集路径：** 节点上日志收集路径 + **metadata：** key：value格式，收集的日志会带上metadata信息上报给消费端 + **消费端：** 选择日志消费端 + **Kafka：** + **访问地址：** kafka ip和端口 + **主题（Topic）：** kafka topic名 + **Elasticsearch：** + **Elasticsearch地址：** ES 地址，如：http://190.0.0.1:200 + **索引：** ES索引，最长60个字符，只能包含小写字母、数字及分隔符(\"-\"、\"_\"、\"+\")，且必须以小写字母开头 5. 单击【完成】按钮。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:14:49 "},"zh/3-产品使用指南/3-1-0-切换控制台.html":{"url":"zh/3-产品使用指南/3-1-0-切换控制台.html","title":"切换控制台","keywords":"","body":"平台管理和业务管理切换 概念 这里用户可以自由切换控制面和业务面。 注意：只有当【平台管理】控制台中创建的业务的成员包含当前登录的用户，当前用户才会出现并可以切换至【业务管理】控制台，如下图所示。 操作步骤 登录 TKEStack，默认显示【平台管理】控制台,鼠标移动到【平台管理】旁，会出现切换提示,如下图： 如果当前显示的是【业务管理】控制台，鼠标移动到【业务管理】旁，会出现切换提示，如下图： 点击【 切换图标】 即可实现【平台管理】和【业务管理】控制台切换。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:50:24 "},"zh/4-产品特色功能/":{"url":"zh/4-产品特色功能/","title":"产品特色功能","keywords":"","body":"产品特色功能 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:08:54 "},"zh/4-产品特色功能/4-1-Galaxy.html":{"url":"zh/4-产品特色功能/4-1-Galaxy.html","title":"Galaxy","keywords":"","body":"Galaxy Kubernetes没有提供默认可用的容器网络，但kubernetes网络的设计文档要求容器网络的实现能做到下面的三点： all containers can communicate with all other containers without NAT all nodes can communicate with all containers (and vice-versa) without NAT the IP that a container sees itself as is the same IP that others see it as 即集群包含的每一个容器都拥有一个与其他集群中的容器和节点可直接路由的独立IP地址。但是Kubernetes并没有具体实现这样一个网络模型，而是实现了一个开放的容器网络标准CNI，可以自由选择使用开源的网络方案或者实现一套遵循CNI标准的网络，为用户提供两种网络类型： Overlay Network，即通用的虚拟化网络模型，不依赖于宿主机底层网络架构，可以适应任何的应用场景，方便快速体验。但是性能较差，因为在原有网络的基础上叠加了一层Overlay网络，封包解包或者NAT对网络性能都是有一定损耗的。 Underlay Network，即基于宿主机物理网络环境的模型，容器与现有网络可以直接互通，不需要经过封包解包或是NAT，其性能最好。但是其普适性较差，且受宿主机网络架构的制约，比如MAC地址可能不够用。 为满足复杂应用容器化的特殊需求，大幅拓展了容器应用的场景，TKEStack利用Galaxy网络组件提供多种解决方案，支持overlay和underlay网络类型，支持高转发性能和高隔离性等场景应用。 Galaxy是一个Kubernetes网络项目，旨在为POD提供通用Overlay和高性能的Underlay网络。 TKEStack使用Galaxy网络组件，支持四种网络模式，并且可以为工作负载单独配置指定的网络模式，拓展了容器应用场景，满足复杂应用容器化的特殊需求。 Overlay网络 TKEStack的默认网络模式，基于IPIP和host gateway的flannel方案，同节点容器通信不走网桥，报文直接利用主机路由转发；跨节点容器通信利用IPIP协议封装, etcd记录节点间路由。该方案控制层简单稳定，网络转发性能优异，并且可以通过network policy实现多种网络策略。 Floating IP 容器IP由宿主机网络提供，打通了容器网络与underlay网络，容器与物理机可以直接路由，性能更好。容器与宿主机的二层连通, 支持了Linux bridge/MacVlan/IPVlan和SRIOV, 根据业务场景和硬件环境，具体选择使用哪种网桥 NAT 基于k8s中的hostPort配置，并且如果用户没有指定Port地址，galaxy会给实例配置容器到主机的随机端口映射 Host 利用k8s中的hostNetwork配置，直接使用宿主机的网络环境，最大的好处是其性能优势，但是需要处理端口冲突问题，并且也有安全隐患。 Galaxy架构 Galaxy在架构上由三部分组成： Galaxy: 以DaemonSet方式运行在每个节点上，通过调用各种CNI插件来配置k8s容器网络 CNI plugins: 符合CNI标准的二进制文件，用于网络资源的配置和管理, 支持CNI插件Supported CNI plugins Galaxy IPAM: 通过tkestack中的IPAM扩展组件安装，K8S调度插件，kube-scheduler通过HTTP调用Galaxy-ipam的filter/priority/bind方法实现Float IP的配置和管理 Galaxy Overlay 网络 tke-installer安装tkestack并自动配置galaxy为overlay网络模式，在该模式下： Flannel在每个Kubelet上分配一个子网，并将其保存在etcd和本地磁盘上(/run/ Flannel /subnet.env) Kubelet根据CNI配置启动SDN CNI进程 1.SDN CNI进程通过unix socket调用Galaxy，所有的args都来自Kubelet Galaxy调用FlannelCNI来解析来自/run/flannel/subnet.env的子网信息 Flannel CNI调用Bridge CNI或Veth CNI来为POD配置网络 Galaxy Underlay 网络 如需配置underlay网络，需要启用Galaxy-ipam组件，Galaxy-ipam根据配置为POD分配或释放IP： 规划容器网络使用的Underlay IP，配置floatingip-config ConfigMap Kubernetes调度器在filter/priority/bind方法上调用Galaxy-ipam Galaxy-ipam检查POD是否配置了reserved IP，如果是，则Galaxy-ipam仅将此IP所在的可用子网的节点标记为有效节点，否则所有都将被标记为有效节点。在POD绑定IP期间，Galaxy-ipam分配一个IP并将其写入到POD annotations中 Galaxy从POD annotations获得IP，并将其作为参数传递给CNI，通过CNI配置POD IP Galaxy配置 Galaxy configuration Galaxy-ipam configuration Float IP usage 常见问题 为pod配置float ip网络模式失败 检查ipam扩展组件是否已正确安装 检查kube-scheduler是否正确配置scheduler-policy 检查floatingip-config ConfigMap是否配置正确 检查创建的Deployment工作负载： 容器限额中配置 tke.cloud.tencent.com/eni-ip:1 容器annotation中配置 k8s.v1.cni.cncf.io/networks=galaxy-k8s-vlan 如果上述配置都正确，pod会被成功创建并运行，galaxy-ipam会自动为pod分配指定的Float IP 为pod配置float ip网络模式后，如何与其他pod和主机通信 Galaxy为pod配置float ip网络模式，pod的nic和ip由宿主机网络提供，此pod的就加入了underlay的网络，因此pod间的通信以及pod与主机的通信就需要网络管理员在相应的交换机和路由器上配置对应的路由。 参考配置 本节展示了在一个正确配置了float-ip的deployment工作负载。 查看kube-scheduler的policy配置文件是否配置正确 # cat /etc/kubernetes/scheduler-policy-config.json { \"apiVersion\" : \"v1\", \"extenders\" : [ { \"apiVersion\" : \"v1beta1\", \"enableHttps\" : false, \"filterVerb\" : \"predicates\", \"managedResources\" : [ { \"ignoredByScheduler\" : false, \"name\" : \"tencent.com/vcuda-core\" } ], \"nodeCacheCapable\" : false, \"urlPrefix\" : \"http://gpu-quota-admission:3456/scheduler\" }, { \"urlPrefix\": \"http://127.0.0.1:32760/v1\", \"httpTimeout\": 10000000000, \"filterVerb\": \"filter\", \"prioritizeVerb\": \"prioritize\", \"BindVerb\": \"bind\", \"weight\": 1, \"enableHttps\": false, \"managedResources\": [ { \"name\": \"tke.cloud.tencent.com/eni-ip\", \"ignoredByScheduler\": true } ] } ], \"kind\" : \"Policy\" } 查看floatingip-config配置 # kubectl get cm -n kube-system floatingip-config -o yaml apiVersion: v1 data: floatingips: '[{\"routableSubnet\":\"172.21.64.0/20\",\"ips\":[\"192.168.64.200~192.168.64.251\"],\"subnet\":\"192.168.64.0/24\",\"gateway\":\"192.168.64.1\"}]' kind: ConfigMap metadata: creationTimestamp: \"2020-03-04T07:09:14Z\" name: floatingip-config namespace: kube-system resourceVersion: \"2711974\" selfLink: /api/v1/namespaces/kube-system/configmaps/floatingip-config uid: 62524e92-f37b-4db2-8ec0-b01d7a90d1a1 查看deployment配置float-ip # kubectl get deploy nnn -o yaml apiVersion: apps/v1 kind: Deployment ... spec: ... template: metadata: annotations: k8s.v1.cni.cncf.io/networks: galaxy-k8s-vlan k8s.v1.cni.galaxy.io/release-policy: immutable creationTimestamp: null labels: k8s-app: nnn qcloud-app: nnn spec: containers: - image: nginx imagePullPolicy: Always name: nnn resources: limits: cpu: 500m memory: 1Gi tke.cloud.tencent.com/eni-ip: \"1\" requests: cpu: 250m memory: 256Mi tke.cloud.tencent.com/eni-ip: \"1\" 查看生成的pod带有float-ip的annotations # kubectl get pod nnn-7df5984746-58hjm -o yaml apiVersion: v1 kind: Pod metadata: annotations: k8s.v1.cni.cncf.io/networks: galaxy-k8s-vlan k8s.v1.cni.galaxy.io/args: '{\"common\":{\"ipinfos\":[{\"ip\":\"192.168.64.202/24\",\"vlan\":0,\"gateway\":\"192.168.64.1\",\"routable_subnet\":\"172.21.64.0/20\"}]}}' k8s.v1.cni.galaxy.io/release-policy: immutable ... spec: ... status: ... hostIP: 172.21.64.15 phase: Running podIP: 192.168.64.202 podIPs: - ip: 192.168.64.202 查看crd中保存的floatingips绑定信息 # kubectl get floatingips.galaxy.k8s.io 192.168.64.202 -o yaml apiVersion: galaxy.k8s.io/v1alpha1 kind: FloatingIP metadata: creationTimestamp: \"2020-03-04T08:28:15Z\" generation: 1 labels: ipType: internalIP name: 192.168.64.202 resourceVersion: \"2744910\" selfLink: /apis/galaxy.k8s.io/v1alpha1/floatingips/192.168.64.202 uid: b5d55f27-4548-44c7-b8ad-570814b55026 spec: attribute: '{\"NodeName\":\"172.21.64.15\"}' key: dp_default_nnn_nnn-7df5984746-58hjm policy: 1 subnet: 172.21.64.0/20 updateTime: \"2020-03-04T08:28:15Z\" 查看所在主机上生成了对应的nic和ip ```shell script ip route default via 172.21.64.1 dev eth0 169.254.0.0/16 dev eth0 scope link metric 1002 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 172.21.64.0/20 dev eth0 proto kernel scope link src 172.21.64.15 ... 192.168.64.202 dev v-hb21e7165d ``` TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:15:30 "},"zh/4-产品特色功能/4-2-TAPP.html":{"url":"zh/4-产品特色功能/4-2-TAPP.html","title":"TAPP","keywords":"","body":"TAPP Kubernetes现有应用类型（如：Deployment、StatefulSet等）无法满足很多非微服务应用的需求，比如：操作（升级、停止等）应用中的指定pod、应用支持多版本的pod。如果要将这些应用改造为适合于这些workload的应用，需要花费很大精力，这将使大多数用户望而却步。 为解决上述复杂应用管理场景，基于Kubernetes CRD开发了一种新的应用类型TAPP，它是一种通用类型的workload，同时支持service和batch类型作业，满足绝大部分应用场景，它能让用户更好的将应用迁移到Kubernetes集群。 TAPP 特点 功能点 Deployment StatefulSet TAPP Pod唯一性 无 每个Pod有唯一标识 每个Pod有唯一标识 Pod存储独占 仅支持单容器 支持 支持 存储随Pod迁移 不支持 支持 支持 自动扩缩容 支持 不支持 支持 批量升级 支持 不支持 支持 严格顺序更新 不支持 支持 不支持 自动迁移问题节点 支持 不支持 支持 多版本管理 同时只有1个版本 可保持2个版本 可保持多个版本 Pod原地升级 不支持 不支持 支持 如果用Kubernetes的应用类型类比，TAPP ≈ Deployment + StatefulSet + Job ，它包含了Deployment、StatefulSet、Job的绝大部分功能，同时也有自己的特性，并且和原生Kubernetes相同的使用方式完全一致。 实例具有可以标识的id 实例有了id，业务就可以将很多状态或者配置逻辑和该id做关联，当容器迁移时，通过TAPP的容器实例标识，可以识别该容器原来对应的数据，实现带云硬盘或者数据目录迁移 每个实例可以绑定自己的存储 通过TAPP的容器实例标识，能很好地支持有状态的作业。在实例发生跨机迁移时，云硬盘能跟随实例一起迁移 实现真正的灰度升级/回退 Kubernetes中的灰度升级概念应为滚动升级，kubernetes将pod”逐个”的更新，但现实中多业务需要的是稳定的灰度，即同一个app，需要有多个版本同时稳定长时间的存在，TAPP解决了此类问题 可以指定实例id做删除、停止、重启等操作 对于微服务app来说，由于没有固定id，因此无法对某个实例做操作，而是全部交给了系统去维持足够的实例数 对每个实例都有生命周期的跟踪 对于一个实例，由于机器故障发生了迁移、重启等操作，很难跟踪和监控其生命周期。通过TAPP的容器实例标识，获得了实例真正的生命周期的跟踪，对于判断业务和系统是否正常服务具有特别重要的意义。TAPP还可以记录事件，以及各个实例的运行时间，状态，高可用设置等。 TAPP 资源结构 TApp定义了一种用户自定义资源（CRD），TAPP controller是TAPP对应的controller/operator，它通过kube-apiserver监听TApp、Pod相关的事件，根据TApp spec和status进行相应的操作：创建、删除pod等。 // TApp represents a set of pods with consistent identities. type TApp struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` // Spec defines the desired identities of pods in this tapp. Spec TAppSpec `json:\"spec,omitempty\"` // Status is the current status of pods in this TApp. This data // may be out of date by some window of time. Status TAppStatus `json:\"status,omitempty\"` } // A TAppSpec is the specification of a TApp. type TAppSpec struct { // Replicas 指定Template的副本数，尽管共享同一个Template定义，但是每个副本仍有唯一的标识 Replicas int32 `json:\"replicas,omitempty\"` // 同Deployment的定义，标签选择器，默认为Pod Template上的标签 Selector *metav1.LabelSelector `json:\"selector,omitempty\"` // Template 默认模板，描述将要被初始创建/默认缩放的pod的对象，在TApp中可以被添加到TemplatePool中 Template corev1.PodTemplateSpec `json:\"template\"` // TemplatePool 描述不同版本的pod template， template name --> pod Template TemplatePool map[string]corev1.PodTemplateSpec `json:\"templatePool,omitempty\"` // Statuses 用来指定对应pod实例的目标状态，instanceID --> desiredStatus [\"Running\",\"Killed\"] Statuses map[string]InstanceStatus `json:\"statuses,omitempty\"` // Templates 用来指定运行pod实例所使用的Template，instanceID --> template name Templates map[string]string `json:\"templates,omitempty\"` // UpdateStrategy 定义滚动更新策略 UpdateStrategy TAppUpdateStrategy `json:\"updateStrategy,omitempty\"` // ForceDeletePod 定义是否强制删除pod，默认为false ForceDeletePod bool `json:\"forceDeletePod,omitempty\"` // 同Statefulset的定义 VolumeClaimTemplates []corev1.PersistentVolumeClaim `json:\"volumeClaimTemplates,omitempty\"` } // 滚动更新策略 type TAppUpdateStrategy struct { // 滚动更新的template name Template string `json:\"template,omitempty\"` // 滚动更新时的最大不可用数, 如果不指定此配置，滚动更新时不限制最大不可用数 MaxUnavailable *int32 `json:\"maxUnavailable,omitempty\"` } // 定义TApp的状态 type TAppStatus struct { // most recent generation observed by controller. ObservedGeneration int64 `json:\"observedGeneration,omitempty\"` // Replicas 描述副本数 Replicas int32 `json:\"replicas\"` // ReadyReplicas 描述Ready副本数 ReadyReplicas int32 `json:\"readyReplicas\"` // ScaleSelector 是用于对pod进行查询的标签，它与HPA使用的副本计数匹配 ScaleLabelSelector string `json:\"scaleLabelSelector,omitempty\"` // AppStatus 描述当前Tapp运行状态, 包含\"Pending\",\"Running\",\"Failed\",\"Succ\",\"Killed\" AppStatus AppStatus `json:\"appStatus,omitempty\"` // Statues 描述实例的运行状态 instanceID --> InstanceStatus [\"NotCreated\",\"Pending\",\"Running\",\"Updating\",\"PodFailed\",\"PodSucc\",\"Killing\",\"Killed\",\"Failed\",\"Succ\",\"Unknown\"] Statuses map[string]InstanceStatus `json:\"statuses,omitempty\"` } 使用示例 本节以一个TApp应用部署，配置，升级，扩容以及杀死删除的操作步骤来说明TApp的使用。 创建TApp应用 创建TApp应用，副本数为3，TApp-controller将根据默认模板创建出的pod $ cat tapp.yaml apiVersion: apps.tkestack.io/v1 kind: TApp metadata: name: example-tapp spec: replicas: 3 template: metadata: labels: app: example-tapp spec: containers: - name: nginx image: nginx:1.7.9 $ kubect apply -f tapp.yaml 查看TApp应用 ```shell script $ kubectl get tapp XXX NAME AGE example-tapp 20m $ kubectl descirbe tapp example-tapp Name: example-tapp Namespace: default Labels: app=example-tapp Annotations: API Version: apps.tkestack.io/v1 Kind: TApp ... Spec: ... Status: App Status: Running Observed Generation: 2 Ready Replicas: 3 Replicas: 3 Scale Label Selector: app=example-tapp Statuses: 0: Running 1: Running 2: Running Events: Type Reason Age From Message Normal SuccessfulCreate 12m tapp-controller Instance: example-tapp-1 Normal SuccessfulCreate 12m tapp-controller Instance: example-tapp-0 Normal SuccessfulCreate 12m tapp-controller Instance: example-tapp-2 ### 升级TApp应用 当前3个pod实例运行的镜像版本为nginx:1.7.9，现在要升级其中的一个pod实例的镜像版本为nginx:latest，在spec.templatPools中创建模板，然后在spec.templates中指定模板pod, 指定“1”:“test”表示使用模板test创建pod 1。 如果只更新镜像，Tapp controller将对pod进行原地升级，即仅更新重启对应的容器，否则将按k8s原生方式删除pod并重新创建它们。 ```yaml apiVersion: apps.tkestack.io/v1 kind: TApp metadata: name: example-tapp spec: replicas: 3 template: metadata: labels: app: example-tapp spec: containers: - name: nginx image: nginx:1.7.9 templatePool: \"test\": metadata: labels: app: example-tapp spec: containers: - name: nginx image: nginx:latest templates: \"1\": \"test\" 操作成功后，查看instanceID为'1'的pod已升级，镜像版本为nginx:latest # kubectl describe tapp example-tapp Name: example-tapp Namespace: default Labels: app=example-tapp Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"apps.tkestack.io/v1\",\"kind\":\"TApp\",\"metadata\":{\"annotations\":{},\"name\":\"example-tapp\",\"namespace\":\"default\"},\"spec\":{\"repli... API Version: apps.tkestack.io/v1 Kind: TApp ... Spec: ... Templates: 1: test Update Strategy: Status: App Status: Running Observed Generation: 4 Ready Replicas: 3 Replicas: 3 Scale Label Selector: app=example-tapp Statuses: 0: Running 1: Running 2: Running Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 25m tapp-controller Instance: example-tapp-1 Normal SuccessfulCreate 25m tapp-controller Instance: example-tapp-0 Normal SuccessfulCreate 25m tapp-controller Instance: example-tapp-2 Normal SuccessfulUpdate 10m tapp-controller Instance: example-tapp-1 # kubectl get pod | grep example-tapp example-tapp-0 1/1 Running 0 27m example-tapp-1 1/1 Running 1 27m example-tapp-2 1/1 Running 0 27m # kubectl get pod example-tapp-1 -o template --template='{{range .spec.containers}}{{.image}}{{end}}' nginx:latest 上述升级过程可根据实际需求灵活操作，可以指定多个pod的版本，帮助用户实现灵活的应用升级策略 同时可以指定updateStrategy升级策略，保证升级是最大不可用数为1，即保证滚动升级时每次仅更新和重启一个容器或pod # cat tapp.yaml apiVersion: apps.tkestack.io/v1 kind: TApp metadata: name: example-tapp spec: replicas: 3 template: metadata: labels: app: example-tapp spec: containers: - name: nginx image: nginx:1.7.9 templatePool: \"test\": metadata: labels: app: example-tapp spec: containers: - name: nginx image: nginx:latest templates: \"1\": \"test\" \"2\": \"test\" \"0\": \"test\" updateStrategy: template: test maxUnavailable: 1 # kubectl apply -f tapp.yaml 杀死指定pod 在spec.statuses中指定pod的状态，tapp-controller根据用户指定的状态控制pod实例，例如，如果spec.statuses为“1”:“killed”，tapp控制器会杀死pod 1。 # cat tapp.yaml kind: TApp metadata: name: example-tapp spec: replicas: 3 template: metadata: labels: app: example-tapp spec: containers: - name: nginx image: nginx:1.7.9 templatePool: \"test\": metadata: labels: app: example-tapp spec: containers: - name: nginx image: nginx:latest templates: \"1\": \"test\" \"2\": \"test\" \"0\": \"test\" updateStrategy: template: test maxUnavailable: 1 statuses: \"1\": \"Killed\" # kubectl apply -f tapp.yaml 查看pod状态变为Terminating ```shell script kubectl get pod NAME READY STATUS RESTARTS AGE example-tapp-0 1/1 Running 1 59m example-tapp-1 0/1 Terminating 1 59m example-tapp-2 1/1 Running 1 59m ### 扩容TApp应用 如果你想要扩展TApp使用默认的spec.template模板，只需增加spec.replicas的值，否则你需要在spec.templates中指定使用哪个模板。kubectl scale也适用于TApp。 ```shell script kubectl scale --replicas=3 tapp/example-tapp 删除TApp应用 shell script kubectl delete tapp example-tapp 其它 Tapp还支持其他功能，如HPA、volume templates，它们与k8s中的其它工作负载类型类似。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:15:43 "},"zh/4-产品特色功能/4-3-GPUManager.html":{"url":"zh/4-产品特色功能/4-3-GPUManager.html","title":"GPU-Manager说明","keywords":"","body":"GPU-Manager说明 组件介绍 GPU Manager提供一个All-in-One的GPU管理器, 基于Kubernets Device Plugin插件系统实现, 该管理器提供了分配并共享GPU, GPU指标查询, 容器运行前的GPU相关设备准备等功能, 支持用户在Kubernetes集群中使用GPU设备。 管理器包含如下功能: 拓扑分配：提供基于GPU拓扑分配功能, 当用户分配超过1张GPU卡的的应用, 可以选择拓扑连接最快的方式分配GPU设备 GPU共享：允许用户提交小于1张卡资源的的任务, 并提供QoS保证 应用GPU指标的查询：用户可以访问主机的端口(默认为5678)的/metrics路径,可以为Prometheus提供GPU指标的收集功能, /usage路径可以提供可读性的容器状况查询 部署在集群内kubernetes对象 在集群内部署GPU-Manager Add-on , 将在集群内部署以下kubernetes对象 kubernetes对象名称 类型 建议预留资源 所属Namespaces gpu-manager-daemonset DaemonSet 每节点1核CPU, 1Gi内存 kube-system gpu-quota-admission Deployment 1核CPU, 1Gi内存 kube-system GPU-Manager使用场景 在Kubernetes集群中运行GPU应用时, 可以解决AI训练等场景中申请独立卡造成资源浪费的情况，让计算资源得到充分利用。 GPU-Manager限制条件 该组件基于Kubernetes DevicePlugin实现, 只能运行在支持DevicePlugin的TKE的1.10kubernetes版本之上。 每张GPU卡一共有100个单位的资源, 仅支持0-1的小数卡,以及1的倍数的整数卡设置. 显存资源是以256MiB为最小的一个单位的分配显存。 使用GPU-Manager 要求集群内包含GPU机型节点。 GPU-Manager使用方法 安装GPU-Manager扩展组件 在安装了GPU-Manager扩展组件的集群中，创建工作负载。 创建工作负载设置GPU限制，如图： yaml创建 如果使用yaml创建工作负载，提交的时候需要在yaml为容器设置GPU的的使用资源, 核资源需要在resource上填写tencent.com/vcuda-core, 显存资源需要在resource上填写tencent.com/vcuda-memory, 使用1张卡 apiVersion: v1 kind: Pod ... spec: containers: - name: gpu resources: tencent.com/vcuda-core: 100 使用0.3张卡, 5GiB显存的应用（20*256MB） apiVersion: v1 kind: Pod ... spec: containers: - name: gpu resources: tencent.com/vcuda-core: 30 tencent.com/vcuda-memory: 20 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:15:54 "},"zh/4-产品特色功能/4-4-CronHPA.html":{"url":"zh/4-产品特色功能/4-4-CronHPA.html","title":"CronHPA","keywords":"","body":"CronHPA Cron Horizontal Pod Autoscaler(CronHPA)使我们能够使用crontab模式定期自动扩容工作负载(那些支持扩展子资源的负载，例如deployment、statefulset)。 CronHPA使用Cron格式进行编写，周期性地在给定的调度时间对工作负载进行扩缩容。 CronHPA 资源结构 CronHPA定义了一个新的CRD，cron-hpa-controller是该CRD对应的controller/operator，它解析CRD中的配置，根据系统时间信息对相应的工作负载进行扩缩容操作。 // CronHPA represents a set of crontabs to set target's replicas. type CronHPA struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` // Spec defines the desired identities of pods in this cronhpa. Spec CronHPASpec `json:\"spec,omitempty\"` // Status is the current status of pods in this CronHPA. This data // may be out of date by some window of time. Status CronHPAStatus `json:\"status,omitempty\"` } // A CronHPASpec is the specification of a CronHPA. type CronHPASpec struct { // scaleTargetRef points to the target resource to scale ScaleTargetRef autoscalingv2.CrossVersionObjectReference `json:\"scaleTargetRef\" protobuf:\"bytes,1,opt,name=scaleTargetRef\"` Crons []Cron `json:\"crons\" protobuf:\"bytes,2,opt,name=crons\"` } type Cron struct { // The schedule in Cron format, see https://en.wikipedia.org/wiki/Cron. Schedule string `json:\"schedule\" protobuf:\"bytes,1,opt,name=schedule\"` TargetReplicas int32 `json:\"targetReplicas\" protobuf:\"varint,2,opt,name=targetReplicas\"` } // CronHPAStatus represents the current state of a CronHPA. type CronHPAStatus struct { // Information when was the last time the schedule was successfully scheduled. // +optional LastScheduleTime *metav1.Time `json:\"lastScheduleTime,omitempty\" protobuf:\"bytes,2,opt,name=lastScheduleTime\"` } // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // CronHPAList is a collection of CronHPA. type CronHPAList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata,omitempty\"` Items []CronHPA `json:\"items\"` } 使用示例 指定deployment每周五20点扩容到60个实例，周日23点缩容到30个实例 apiVersion: extensions.tkestack.io/v1 kind: CronHPA metadata: name: example-cron-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: demo-deployment crons: - schedule: \"0 20 * * 5\" targetReplicas: 60 - schedule: \"0 23 * * 7\" targetReplicas: 30 指定deployment每天8点到9点，19点到21点扩容到60，其他时间点恢复到10 apiVersion: extensions.tkestack.io/v1 kind: CronHPA metadata: name: web-servers-cronhpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: web-servers crons: - schedule: \"0 8 * * *\" targetReplicas: 60 - schedule: \"0 9 * * *\" targetReplicas: 10 - schedule: \"0 19 * * *\" targetReplicas: 60 - schedule: \"0 21 * * *\" targetReplicas: 10 查看cronhpa ```shell script kubectl get cronhpa NAME AGE example-cron-hpa 104s kubectl get cronhpa example-cron-hpa -o yaml apiVersion: extensions.tkestack.io/v1 kind: CronHPA ... spec: crons: schedule: 0 20 5 targetReplicas: 60 schedule: 0 23 7 targetReplicas: 30 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: demo-deployment ### 删除cronhpa ```shell script kubectl delete cronhpa example-cron-hpa TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:16:02 "},"zh/4-产品特色功能/4-5-LBCF.html":{"url":"zh/4-产品特色功能/4-5-LBCF.html","title":"LBCF说明","keywords":"","body":"LBCF说明 组件介绍 : Load Balancer Controlling Framework (LBCF) LBCF是一款部署在Kubernetes内的通用负载均衡控制面框架，旨在降低容器对接负载均衡的实现难度，并提供强大的扩展能力以满足业务方在使用负载均衡时的个性化需求。 部署在集群内kubernetes对象 在集群内部署LBCF Add-on , 将在集群内部署以下kubernetes对象 kubernetes对象名称 类型 默认占用资源 所属Namespaces lbcf-controller Deployment / kube-system lbcf-controller ServiceAccount / kube-system lbcf-controller ClusterRole / / lbcf-controller ClusterRoleBinding / / lbcf-controller Secret / kube-system lbcf-controller Service / kube-system backendrecords.lbcf.tkestack.io CustomResourceDefinition / / backendgroups.lbcf.tkestack.io CustomResourceDefinition / / loadbalancers.lbcf.tkestack.io CustomResourceDefinition / / loadbalancerdrivers.lbcf.tkestack.io CustomResourceDefinition / / lbcf-mutate MutatingWebhookConfiguration / / lbcf-validate ValidatingWebhookConfiguration / / LBCF使用场景 LBCF对K8S内部晦涩的运行机制进行了封装并以Webhook的形式对外暴露，在容器的全生命周期中提供了多达8种Webhook。通过实现这些Webhook，开发人员可以轻松实现下述功能： 对接任意负载均衡/名字服务，并自定义对接过程 实现自定义灰度升级策略 容器环境与其他环境共享同一个负载均衡 解耦负载均衡数据面与控制面 LBCF使用方法 通过扩展组件安装LBCF 开发或选择安装LBCF Webhook规范的要求实现Webhook服务器 以下按腾讯云CLB开发的webhook服务器为例 详细的使用方法和帮助文档，请参考lb-controlling-framework文档 使用示例 使用已有四层CLB 本例中使用了id为lb-7wf394rv的负载均衡实例，监听器为四层监听器，端口号为20000，协议类型TCP。 注: 程序会以端口号20000，协议类型TCP为条件查询监听器，若不存在，会自动创建新的 apiVersion: lbcf.tkestack.io/v1beta1 kind: LoadBalancer metadata: name: example-of-existing-lb namespace: kube-system spec: lbDriver: lbcf-clb-driver lbSpec: loadBalancerID: \"lb-7wf394rv\" listenerPort: \"20000\" listenerProtocol: \"TCP\" ensurePolicy: policy: Always 创建新的七层CLB 本例在vpc vpc-b5hcoxj4中创建了公网(OPEN)负载均衡实例，并为之创建了端口号为9999的HTTP监听器，最后会在监听器中创建mytest.com/index.html的转发规则 apiVersion: lbcf.tkestack.io/v1beta1 kind: LoadBalancer metadata: name: example-of-create-new-lb namespace: kube-system spec: lbDriver: lbcf-clb-driver lbSpec: vpcID: vpc-b5hcoxj4 loadBalancerType: \"OPEN\" listenerPort: \"9999\" listenerProtocol: \"HTTP\" domain: \"mytest.com\" url: \"/index.html\" ensurePolicy: policy: Always 设定backend权重 本例展示了Service NodePort的绑定。被绑定Service的名称为svc-test，service port为80（TCP)，绑定到CLB的每个Node:NodePort的权重都是66 apiVersion: lbcf.tkestack.io/v1beta1 kind: BackendGroup metadata: name: web-svc-backend-group namespace: kube-system spec: lbName: test-clb-load-balancer service: name: svc-test port: portNumber: 80 parameters: weight: \"66\" 附录 腾讯云CLB LBCF driver ConfigMap： apiVersion: v1 kind: ConfigMap metadata: name: trusted-tencentcloudapi namespace: kube-system data: tencentcloudapi.pem: | -----BEGIN CERTIFICATE----- ............. -----END CERTIFICATE----- Deployment apiVersion: lbcf.tkestack.io/v1beta1 kind: LoadBalancerDriver metadata: name: lbcf-clb-driver namespace: kube-system spec: driverType: Webhook url: \"http://lbcf-clb-driver.kube-system.svc\" --- apiVersion: apps/v1 kind: Deployment metadata: name: lbcf-clb-driver namespace: kube-system spec: replicas: 1 selector: matchLabels: lbcf.tkestack.io/component: lbcf-clb-driver template: metadata: labels: lbcf.tkestack.io/component: lbcf-clb-driver spec: priorityClassName: \"system-node-critical\" containers: - name: driver image: ${image-name} args: - \"--region=${your-region}\" - \"--vpc-id=${your-vpc-id}\" - \"--secret-id=${your-account-secret-id}\" - \"--secret-key=${your-account-secret-key}\" ports: - containerPort: 80 name: insecure imagePullPolicy: Always volumeMounts: - name: trusted-ca mountPath: /etc/ssl/certs readOnly: true volumes: - name: trusted-ca configMap: name: trusted-tencentcloudapi Service: apiVersion: v1 kind: Service metadata: labels: name: lbcf-clb-driver namespace: kube-system spec: ports: - name: insecure port: 80 targetPort: 80 selector: lbcf.tkestack.io/component: lbcf-clb-driver sessionAffinity: None type: ClusterIP TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:16:09 "},"zh/5-FAQ/":{"url":"zh/5-FAQ/","title":"FAQ","keywords":"","body":"FAQ TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:09:14 "},"zh/5-FAQ/5-1-部署类/":{"url":"zh/5-FAQ/5-1-部署类/","title":"部署类","keywords":"","body":"部署类 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:09:32 "},"zh/5-FAQ/5-1-部署类/5-1-1-如何规划部署资源.html":{"url":"zh/5-FAQ/5-1-部署类/5-1-1-如何规划部署资源.html","title":"如何规划部署资源","keywords":"","body":"如何规划部署资源 TKEStack支持使用物理机或虚拟机部署，采用kubernetes on kubernetes架构部署，在主机上只拥有一个物理机进程kubelet，其他kubernetes组件均为容器。架构上分为global集群和业务集群。global集群，运行整个TKEStack平台自身所需要的组件，业务集群运行用户业务。在实际的部署过程中，可根据实际情况进行调整。 安装TKEStack，需要提供两种角色的 Server： Installer server 1台，用以部署集群安装器，安装完成后可以回收。 Global server，若干台，用以部署 Globa 集群，常见的部署模式分为三种： All in one 模式，1台server部署 Global集群，global集群同时也充当业务集群的角色，即运行平台基础组件，又运行业务容器。global集群会默认设置taint不可调度，使用此模式时，需要手工在golbal集群【节点管理】-【更多】-【编辑Taint】中去除不可调度设置。(关于taint，了解更多)。由于此种模式不具有高可用能力，不建议在生产环境中使用。 Global 与业务集群混部的高可用模式，3台Server部署global集群，global集群同时也充当业务集群的角色，即运行平台基础组件，又运行业务容器。global集群会默认设置taint不可调度，使用此模式时，需要手工在golbal集群【节点管理】-【更多】-【编辑Taint】中去除不可调度设置。(关于taint，了解更多)。由于此种模式有可能因为业务集群资源占用过高而影响global集群，不建议在生产环境中使用。 Global 与业务集群分别部署的高可用模式，3台Server部署global集群，仅运行平台自身组件，业务集群单独在TKEStack控制台上创建（建议3台以上），此种模式下，业务资源占有与平台隔离，建议在生产环境中使用此种模式。 集群节点主机配置，请参考资源需求。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:17:43 "},"zh/5-FAQ/5-1-部署类/5-1-2-如何使用存储.html":{"url":"zh/5-FAQ/5-1-部署类/5-1-2-如何使用存储.html","title":"如何使用存储","keywords":"","body":"如何使用存储 TKEStack 没有提供存储服务，Global集群中的镜像仓库、ETCD、InfluxDB等数据组件，均使用本地磁盘存储数据。如果您需要使用存储服务，建议使用ROOK或者chubaoFS，部署一套容器化的分布式存储服务。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:18:03 "},"zh/5-FAQ/5-1-部署类/5-1-3-如何重新部署.html":{"url":"zh/5-FAQ/5-1-部署类/5-1-3-如何重新部署.html","title":"常见报错解决方案","keywords":"","body":"常见报错解决方案 安装过程中的错误主要集中在硬件和软件配置上。 首先检查Global节点是否满足8核16G内存，100G系统盘的要求。 其次仔细检查每个节点的硬件和软件需求：installation requirements 密码安装报错 错误情况：使用密码安装Global集群报 ssh:unable to authenticate 错误。 解决方案：将Global集群节点/etc/ssh/sshd_config配置文件中的PasswordAuthentication设为yes，重启sshd服务。 注：建议配置SSH key的方式安装Global集群。 如何重新部署集群 继续安装 若安装报错后，请先排障，再登录到 Installer 节点执行如下命令后，重新打开 http://[tke-installer-IP]:8080/index.html 安装控制台。 docker restart tke-installer 重新安装 安装报错后，请先排障，再登录到 Installer 节点执行如下命令后，重新打开 http://[tke-installer-IP]:8080/index.html 安装控制台。 rm -rf /opt/tke-installer/data && docker restart tke-installer 彻底清除所有安装文件，重新部署TKEStack。 想要彻底清理TKEStack，请对installer和所有Global Cluster节点执行下方脚本。 curl -s https://tke-release-1251707795.cos.ap-guangzhou.myqcloud.com/tools/clean.sh | sh 或者使用如下脚本： #!/bin/bash rm -rf /etc/kubernetes systemctl stop kubelet 2>/dev/null docker rm -f $(docker ps -aq) 2>/dev/null systemctl stop docker 2>/dev/null ip link del cni0 2>/etc/null for port in 80 2379 6443 8086 {10249..10259} ; do fuser -k -9 ${port}/tcp done rm -rfv /etc/kubernetes rm -rfv /etc/docker rm -fv /root/.kube/config rm -rfv /var/lib/kubelet rm -rfv /var/lib/cni rm -rfv /etc/cni rm -rfv /var/lib/etcd rm -rfv /var/lib/postgresql /etc/core/token /var/lib/redis /storage /chart_storage systemctl start docker 2>/dev/null 注：如有混合部署其他业务，请基于实际情况评估目录内数据是否可删除。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:18:10 "},"zh/5-FAQ/5-2-功能类/":{"url":"zh/5-FAQ/5-2-功能类/","title":"功能类","keywords":"","body":"功能类 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:09:48 "},"zh/5-FAQ/5-2-功能类/5-2-2-如何实现自定义监控.html":{"url":"zh/5-FAQ/5-2-功能类/5-2-2-如何实现自定义监控.html","title":"如何实现自定义监控","keywords":"","body":"如何实现自定义监控 TKEStack 提供了的默认监控内容为工作负载的资源监控，监控功能将会持续拓展，您也可以通过部署prometheus-operator来实现自定义指标的监控。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:18:54 "},"zh/5-FAQ/5-2-功能类/5-2-3-如何做日志分析.html":{"url":"zh/5-FAQ/5-2-功能类/5-2-3-如何做日志分析.html","title":"如何做日志分析","keywords":"","body":"如何做日志分析 为集群开启日志采集功能后，在【运维中心】中配置【日志采集规则】，将日志输出。。。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:19:09 "},"zh/5-FAQ/5-3-授权类/":{"url":"zh/5-FAQ/5-3-授权类/","title":"授权类","keywords":"","body":"授权类 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:10:03 "},"zh/5-FAQ/5-3-授权类/5-3-1-业务管理与平台管理的区别.html":{"url":"zh/5-FAQ/5-3-授权类/5-3-1-业务管理与平台管理的区别.html","title":"业务管理、平台管理的区别","keywords":"","body":"业务管理、平台管理的区别 TKEStack的权限体系分为业务使用者和平台管理员两种角色，平台管理员可以管理平台所有功能，业务使用者可以访问自己有权限的业务或者namespace下的资源。同时平台管理员可以通过自定义策略，定义不同的策略类型。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:19:47 "},"zh/5-FAQ/5-3-授权类/5-3-2-如何设置自定义策略.html":{"url":"zh/5-FAQ/5-3-授权类/5-3-2-如何设置自定义策略.html","title":"如何设置自定义策略","keywords":"","body":"如何设置自定义策略 TKEStack 策略（policy）用来描述授权的具体信息。核心元素包括操作（action）、资源（resource）以及效力（effect）。 操作（action） 描述允许或拒绝的操作。操作可以是 API（以 name 前缀描述）或者功能集（一组特定的 API，以 permid 前缀描述）。该元素是必填项。 资源（resource） 描述授权的具体数据。资源是用六段式描述。每款产品的资源定义详情会有所区别。有关如何指定资源的信息，请参阅您编写的资源声明所对应的产品文档。该元素是必填项。 效力（effect） 描述声明产生的结果是“允许”还是“显式拒绝”。包括 allow（允许）和 deny （显式拒绝）两种情况。该元素是必填项。 策略样例 该样例描述为：允许关联到此策略的用户，对cls-123集群下的工作负载deploy-123中的所有资源，有查看权限。 { \"actions\": [ \"get*\", \"list*\", \"watch*\" ], \"resources\": [ \"cluster:cls-123/deployment:deploy-123/*\" ], \"effect\": \"allow\" } TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:19:58 "},"zh/5-FAQ/5-3-授权类/5-3-3-Docker-login权限错误.html":{"url":"zh/5-FAQ/5-3-授权类/5-3-3-Docker-login权限错误.html","title":"Docker login 权限错误","keywords":"","body":"Docker login 权限错误 在Tkestack选用用了自建证书，需要用户在客户端手动导入，docker login 权限报错：certificate signed by unknown authority。 方法一 在 Global 集群上执行 kubectl get cm certs -n tke -o yaml 将 ca.crt 内容保存到客户端节点的/etc/docker/certs.d/**/ca.crt ( 为镜像仓库地址) 重启docker即可 方法二： 在/etc/docker/daemon.json文件里添加insecure-registries，如下： { \"insecure-registries\": [ \"xxx\",\"xxx\" ] } （* 为镜像仓库地址） 重启docker即可 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:20:06 "},"zh/5-FAQ/5-4-事件类/":{"url":"zh/5-FAQ/5-4-事件类/","title":"事件类","keywords":"","body":"事件类 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:10:25 "},"zh/5-FAQ/5-4-事件类/5-4-1-常见错误事件.html":{"url":"zh/5-FAQ/5-4-事件类/5-4-1-常见错误事件.html","title":"Back-off restarting failed docker container","keywords":"","body":"Back-off restarting failed docker container 说明：正在重启异常的 Docker 容器。 解决方法：检查镜像中执行的 Docker 进程是否异常退出，若镜像内并无一持续运行的进程，可在创建服务的页面中添加执行脚本。 fit failure on node: Insufficient cpu 说明：集群 CPU 不足。 解决方法：原因是节点无法提供足够的计算核心，请在服务页面修改 CPU 限制或者对集群进行扩容。 no nodes available to schedule pods 说明：集群资源不足。 解决方法：原因是没有足够的节点用于承载实例，请在服务页面修改服务的实例数量，修改实例数量或者 CPU 限制。 pod failed to fit in any node 说明：没有合适的节点可供实例使用。 解决方法：原因是服务配置了不合适的资源限制，导致没有合适的节点用于承载实例，请在服务页面修改服务的实例数量或者 CPU 限制。 Liveness probe failed 说明：容器健康检查失败 解决方法：检查镜像内容器进程是否正常，检查检测端口是否配置正确。 Error syncing pod, skipping Error syncing pod, skipping failed to \"StartContainer\" for with CrashLoopBackOff: \"Back-off 5m0s restarting failed container 说明：容器进程崩溃或退出。 解决方法：检查容器内是否有持续运行的前台进程，若有检查其是否有异常行为。详情请参考 如何构建Docker 镜像。 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 16:21:04 "},"zh/5-FAQ/5-5-平台类/":{"url":"zh/5-FAQ/5-5-平台类/","title":"平台类","keywords":"","body":"平台类 TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-24 17:10:39 "},"zh/5-FAQ/5-5-平台类/平台使用常见问题.html":{"url":"zh/5-FAQ/5-5-平台类/平台使用常见问题.html","title":"平台使用常见问题.md","keywords":"","body":"TKEStack 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-08-20 15:33:35 "}}